<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-06-03T02:13:27+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Wi-tech’s Blog</title><subtitle>A blog post about technical stuff, especially machine learning by an IT student from International University</subtitle><entry><title type="html">Vietnam AQI Visualization</title><link href="http://localhost:4000/aqi-viz" rel="alternate" type="text/html" title="Vietnam AQI Visualization" /><published>2021-06-01T00:00:00+07:00</published><updated>2021-06-01T00:00:00+07:00</updated><id>http://localhost:4000/aqi-viz</id><content type="html" xml:base="http://localhost:4000/aqi-viz">&lt;!--&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &lt;title&gt;Vietnam's AQI&lt;/title&gt;
    &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot;&gt; 
    &lt;link href=&quot;https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700&amp;display=swap&quot; rel=&quot;stylesheet&quot;&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;assets/style.css&quot;&gt;
&lt;/head&gt;--&gt;
&lt;body&gt;
    &lt;div id=&quot;main-page&quot;&gt;
        &lt;div id=&quot;title&quot; class=&quot;bar-graph-panel&quot;&gt;
            &lt;h1&gt;Vietnam's air condition&lt;/h1&gt;
        &lt;/div&gt;
        &lt;div id=&quot;world-rank&quot; class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;How polluted is Vietnam's air in the world?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Vietnam stands at position 21st among the most 100 air-polluted countries in 2020.&lt;/p&gt;
            &lt;br /&gt;
            &lt;div class=&quot;year-option-wrapper&quot; id=&quot;world&quot;&gt;
                &lt;div class=&quot;year-option&quot;&gt;2018&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2019&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2020&lt;/div&gt;
            &lt;/div&gt;
            &lt;svg id=&quot;rank-world-graph&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        
        &lt;div id=&quot;asean-rank&quot; class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;How polluted is Vietnam's air in ASEAN?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Vietnam stands at position 3rd among ASEAN countries in terms of AQI in 2020.&lt;/p&gt;
            &lt;br /&gt;
            &lt;div class=&quot;year-option-wrapper&quot; id=&quot;asean&quot;&gt;
                &lt;div class=&quot;year-option&quot;&gt;2018&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2019&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2020&lt;/div&gt;
            &lt;/div&gt;
            &lt;svg id=&quot;rank-asia-graph&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        
        &lt;div class=&quot;map-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;AQI of some cities in Vietnam&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Most cities in Vietnam belongs to moderate and unhealthy groups.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;map&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;Why is it the case?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Is it due to the reduction of forest area? No! The forest area is still increasing.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;forest&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;Why is it the case?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;However, the amount of carbon dioxide (CO2) is still increasing.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;co2&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;Why is it the case?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;The only reason is from the human activity. Below is the chart for the amount of CO2 emission from three sources: oil (red), gas (blue), coal (green).&lt;/p&gt;
            &lt;p&gt;Vietnam releases at least 10 million tons of CO2 each year. Not surprisingly, coal emits the most amount of CO2 for it is used to produce electricity and gasoline.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;human&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;What are possible solutions?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;The only solution is to less depend on fossil fuel products. For individuals, this can be:&lt;/p&gt;
            &lt;div class=&quot;solution-wrapper&quot;&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://images.vexels.com/media/users/3/128905/isolated/preview/b04ee8fc260d4c6918b67e960ae3b8f5-tour-bus-silhouette-by-vexels.png&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;
                    &lt;p&gt;Using public transports.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Emoji_u2600.svg/1200px-Emoji_u2600.svg.png&quot; alt=&quot;&quot; width=&quot;150&quot; height=&quot;150&quot; /&gt;
                    &lt;p&gt;Utilize solar and other renewable energy.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://www.freeiconspng.com/thumbs/fuel-icon/fuel-pump-icon-23.png&quot; alt=&quot;&quot; width=&quot;150&quot; height=&quot;150&quot; /&gt;
                    &lt;p&gt;Using environmental-friendly fuels (such as bilogical fuels).&lt;/p&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;What are possible solutions?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;For government, this can be:&lt;/p&gt;
            &lt;div class=&quot;solution-wrapper&quot;&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://icons-for-free.com/iconfiles/png/512/svg+lab+microscope+science+icon-1320190754102964758.png&quot; alt=&quot;&quot; /&gt;
                    &lt;p&gt;Research and discover more efficient energy sources.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://images.vexels.com/media/users/3/128905/isolated/preview/b04ee8fc260d4c6918b67e960ae3b8f5-tour-bus-silhouette-by-vexels.png&quot; alt=&quot;&quot; /&gt;
                    &lt;p&gt;Improve transport systems and infrastructures.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://icon-library.com/images/tax-icon-png/tax-icon-png-10.jpg&quot; alt=&quot;&quot; /&gt;
                    &lt;p&gt;Implement environmental taxes.&lt;/p&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;div id=&quot;footer&quot;&gt;
                &lt;p&gt;
                    This project is a small project conducted by a group of students 
                    from the International University, so it is not funded by any organizations.
                &lt;/p&gt;
                &lt;p&gt;
                    Full source for this project is published 
                    &lt;a href=&quot;https://github.com/minhrongcon2000/vn-aqi-viz&quot; target=&quot;_blank&quot;&gt;
                        here
                    &lt;/a&gt;
                &lt;/p&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;script src=&quot;https://d3js.org/d3.v6.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;https://d3js.org/d3-path.v2.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;https://d3js.org/d3-shape.v2.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/scrollytell.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/utils.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/create_map.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/create_bar_chart.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/create_line_chart.js&quot;&gt;&lt;/script&gt;
&lt;/body&gt;</content><author><name>Pham Hoang Minh</name></author><category term="data-visualization" /><summary type="html">Vietnam's air condition How polluted is Vietnam's air in the world? Vietnam stands at position 21st among the most 100 air-polluted countries in 2020. 2018 2019 2020 How polluted is Vietnam's air in ASEAN? Vietnam stands at position 3rd among ASEAN countries in terms of AQI in 2020. 2018 2019 2020 AQI of some cities in Vietnam Most cities in Vietnam belongs to moderate and unhealthy groups. Why is it the case? Is it due to the reduction of forest area? No! The forest area is still increasing. Why is it the case? However, the amount of carbon dioxide (CO2) is still increasing. Why is it the case? The only reason is from the human activity. Below is the chart for the amount of CO2 emission from three sources: oil (red), gas (blue), coal (green). Vietnam releases at least 10 million tons of CO2 each year. Not surprisingly, coal emits the most amount of CO2 for it is used to produce electricity and gasoline. What are possible solutions? The only solution is to less depend on fossil fuel products. For individuals, this can be: Using public transports. Utilize solar and other renewable energy. Using environmental-friendly fuels (such as bilogical fuels). What are possible solutions? For government, this can be: Research and discover more efficient energy sources. Improve transport systems and infrastructures. Implement environmental taxes. This project is a small project conducted by a group of students from the International University, so it is not funded by any organizations. Full source for this project is published here</summary></entry><entry><title type="html">Embrace the likelihood</title><link href="http://localhost:4000/embrace-the-likelihood" rel="alternate" type="text/html" title="Embrace the likelihood" /><published>2020-06-14T00:00:00+07:00</published><updated>2020-06-14T00:00:00+07:00</updated><id>http://localhost:4000/embrace-the-likelihood</id><content type="html" xml:base="http://localhost:4000/embrace-the-likelihood">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The very first machine learning model that one would construct when starting his or her journey in the field of Machine Learning is Linear Regression, an algorithm to find the best linear model to fit a data set. In Machine Learning approach, Linear Regression has five main steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize a weight vector&lt;/li&gt;
  &lt;li&gt;Pass the feature matrix to the model&lt;/li&gt;
  &lt;li&gt;Calculate the mean-squared-error (MSE) loss function:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}\label{eq:1}
	J(w) = \frac{1}{2m}\sum_{i=1}^m (h_w(x) - y)^2 
\end{align}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Calculate its gradient vector $\vec{\nabla} J$&lt;/li&gt;
  &lt;li&gt;Update the weight by using gradient descent to minimize the loss function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, the way to derive the loss function is usually ommitted. It turns out to be a very interesting topic that most of the statistical textbook have overlooked: the difference between likelihood and probability.&lt;/p&gt;

&lt;h1 id=&quot;probability-vs-likelihood-frequentist-vs-bayesian&quot;&gt;Probability vs Likelihood: Frequentist vs Bayesian&lt;/h1&gt;
&lt;p&gt;There is an interesting fact that there was a conflict between these two ideologies. Frequentist was popular in the 18th-19th century since mathematics of this time is purely based on theory. Bayesian approach started gaining popularity recently since a lot of applied mathematical model in the modern day requires the incorporation of uncertainty in it.&lt;/p&gt;

&lt;p&gt;Frequentist approach is, in fact, the very first thing that a beginner in Probability and Statistics would tackle. At the high level, Frequentist approach emphasizes on the proportion of occurrence of events without taking any outside effect into account. For example, in Frequentist approach, the chance that a fair coin landing on head is 1/2 since there are two possible outcomes: head and tail, and landing on each one of them is equally likely to take place. At the low level, Frequentist apprach belives that the chance of occurrence of an event is defined as follow.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A) = \lim_{n\to\infty}\frac{n(A)}{n}&lt;/script&gt;

&lt;p&gt;where $n(A)$ is the number of times the event $A$ happens in the total of $n$ trials.&lt;/p&gt;

&lt;p&gt;Frequentist calls this chance probability. Frequentist approach has been a fundamental building block for various theory in Statistics, one of which is probability distribution such as the famous Gaussian (or so-called normal) distribution, and the Binomial distribution (which can be considered a discrete version of the former).&lt;/p&gt;

&lt;p&gt;The weakness of Frequentist approach is that it does not take external information into account. A coin in Frequentist approach is always a fair coin, which never exists in the real world scenario. Bayesian approach is derived to tackle the issue. Taking the coin tossing problem as an example, in Frequentist approach, its probability of landing on head or tail is assigned to be 1/2. In Bayesian approach, however, the chance that the coin lands on head is $p$, and lands on tail is $1-p$, where $p$ is a random variable. Bayesian approach finds the optimal $q$ based on the data of a certain number of coin tossing. For instance, a coin is tossed 10 times, in which it lands on head 7 times. Frequentist approach would judge that the sample size is not sufficiently large, and would result in doing exhaustive experiments. Bayesian approach assumes that we can only draw that limited amount of sample, and perform inference on $p$.&lt;/p&gt;

&lt;p&gt;Let $A$ be the event that a coin lands on head 7 times out of 10 tosses. Since $p$ is a random variable, it follows some kind of probability distribution in Frequentist approach. In Bayesian approach, we define a prior belief (or a prior probability distribution) on $p$, so we assume that $p$ follows a uniform distribution between 0 and 1. Then, by using the Bayes theorem,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}\label{eq:2}
	f(p|A) = \frac{f(A|p)\pi (p)}{\int_0^1 f(A|p)\pi (p)dp}
\end{align}&lt;/script&gt;

&lt;p&gt;where, in Bayesian terms, $f(p|A)$ is the posterior distribution of $p$ given $A$, $f(A|p)$ is the likelihood that the event $A$ will occur given the probability of landing on head is $p$, $\pi (p)$ is the prior distribution of $p$.&lt;/p&gt;

&lt;p&gt;In Frequentist approach, given that the probability of landing on head is $p$, the chance of $A$ follows a binomial distribution with the proportion of success is $p$; thus, $f(A|p) = \binom{10}{7}p^7(1-p)^3$. Since we assume that $\pi (p)$ is a uniform distribution between 0 and 1, $\pi (p)=1,\text{ }\forall p\in [0,1]$. Since the denominator of (\ref{eq:2}) is always a constant for all p in $[0,1]$, we can rewrite (\ref{eq:2}) as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
	f(p|A) = C\times f(A|p)\times\pi (p) = K\times p^7(1-p)^3
\end{align}&lt;/script&gt;

&lt;p&gt;where $C$ and $K$ are constants.&lt;/p&gt;

&lt;p&gt;Our goal is to find $p$ such that the occurrence of the event is maximize. In other words, given the event $A$ happened, we want to find $p*$ such that $f(p|A)$ reaches its global maxima. Using Calculus approach, the optimal $p$ in this case is $0.7$.&lt;/p&gt;

&lt;p&gt;In comparison to Frequentist approach, Bayesian approach introduces a new concept in conducting inferences: likelihood. Frequentists assume that the chance of an event happening is derived through repeated experiments and call it probability, whereas Bayesians believe that the chance of an event happening is also a random variable following a probability distribution, and finds the optimal chance of occurence through the data drawn from experiment with specifying their prior belief over the occurence chance. Sometimes, prior belief in Bayesian approach does have a major effect in conveying inference on the considered random variable. The methods performed above is called Maximum Likelihood Estimation, and it has been widely used in a lot of statistical textbooks without explaining its origin. It is the basics to derive the mean-squared-error loss function for linear regression.&lt;/p&gt;

&lt;h1 id=&quot;from-maximum-likelihood-estimation-mle-to-linear-regression&quot;&gt;From Maximum Likelihood Estimation (MLE) to Linear Regression&lt;/h1&gt;

&lt;p&gt;We consider a general regression problem as follows: suppose that a vector of random variable $\vec{X}$ and a random variable $Y$ have the relationship of $Y=f(\vec{X})$ where $f$ is a continuous function. To find $f$, we draw a lot of samples of $\vec{X}$ and $Y$ from their distribution, and perform inference on the drawn samples, which is exactly the Bayesian apprach. We denote $\mathcal{D} = \{(\vec{x_i}, y_i) | i=1,2,\ldots,m\}$ as the set of all drawn samples, where $\vec{x_i}\in\mathbb{R}^n$ (n is so-called the number of features), $y_i\in\mathbb{R}^n$, and $y_i = f(x_i) + \epsilon_i$, and $\epsilon_i\sim p(\epsilon)$.&lt;/p&gt;

&lt;p&gt;The common approach to this problem is, first, to assume that $f$ is a member of a family of function $\mathcal{F}$, and second, to find the optimal $f^*\in\mathcal{F}$ that best fits $\mathcal{D}$. In order to define the criteria to evaluate how well the function $f$ fits $\mathcal{D}$, the maximum likelihood estimation (MLE) method is used.&lt;/p&gt;

&lt;p&gt;Our goal is to maximize the likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}\label{eq:3}
P(Y|X) = \prod_{i=1}^m p(y_i|\vec{x_i}) = \prod_{i=1}^m p(\epsilon_i)
\end{align}&lt;/script&gt;

&lt;p&gt;since knowing the appearance of $\vec{x_i}$ will result in the prediction $\hat{y_i} = f(x_i)$, from which $\epsilon_i$ is calculated.&lt;/p&gt;

&lt;p&gt;The mean-squared-error is derived from the assumption that $\epsilon\sim\mathcal{N} (0, \sigma^2)$. With that assumption, by taking natural logarithm both sides (\ref{eq:3}), it can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}\label{eq:4}
\ln P(Y|X) = C - \sum_{i=1}^m \frac{(y_i - f(\vec{x_i}))^2}{2\sigma^2}
\end{align}&lt;/script&gt;

&lt;p&gt;where $C$ is a constant.&lt;/p&gt;

&lt;p&gt;By maximizing $(\ref{eq:4})$, the term $J = \displaystyle{\sum_{i=1}^m} (y_i - f(\vec{x_i}))^2$ is also minimized (since $\sigma^2$ is assumed to be a constant), which is the famous mean-squared-error in Linear Regression.&lt;/p&gt;

&lt;p&gt;From here, one may ask whether another loss function is derived by using this approach, by making different assumption on the distribution of $\epsilon$. The answer is yes; in fact, there is a kind of loss function starting to gain popularity in Linear Regression called the mean-absolute error&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
J = \sum_{i=1}^m |y_i - f(\vec{x_i})|
\end{align*}&lt;/script&gt;

&lt;p&gt;by assuming that $\epsilon$ follows Laplace distribution with mean 0. The only constraint in our assumption here is that the mean of the distribution to be assumed must be 0 (if the mean is not zero, we can shift the distribution to the left or the right based on the positivity or negativity of the mean).&lt;/p&gt;

&lt;p&gt;With this approach, the cross-entropy loss function of Logistic Regression can also be derived. The only difference is that Logistic Regression assume its model as $g(\vec{x_i}) = \frac{1}{1+\exp\{-f(\vec{x_i})\}}$, where $f$ is called the boundary decision. The derivation is left for the reader, and the derivation $g$ will be ommitted since it is out of the scope of this blog.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The title of this blog is to emphasize to the role of likelihood in the field of Machine Learning, a concept that has been overlooked in a lot of basic statistical material. This blog has clarified the difference between Frequentist and Bayesian approach, and has linked a lot of familiar concept in Statistics to Bayesian one. It also shows an application in one of the most popular, yet fundamental algorithm in the field of Machine Learning, in general, and of Deep Learning, in specific, which provides a building block for the derivation of loss function for new model development.&lt;/p&gt;</content><author><name>Pham Hoang Minh</name></author><category term="statistics" /><category term="likelihood" /><category term="bayesian" /><category term="frequentist" /><summary type="html">Introduction The very first machine learning model that one would construct when starting his or her journey in the field of Machine Learning is Linear Regression, an algorithm to find the best linear model to fit a data set. In Machine Learning approach, Linear Regression has five main steps: Initialize a weight vector Pass the feature matrix to the model Calculate the mean-squared-error (MSE) loss function:</summary></entry><entry><title type="html">Toán học đằng sau thuật toán Gradient Descent.</title><link href="http://localhost:4000/gradient-descent" rel="alternate" type="text/html" title="Toán học đằng sau thuật toán Gradient Descent." /><published>2020-06-02T00:00:00+07:00</published><updated>2020-06-02T00:00:00+07:00</updated><id>http://localhost:4000/gradient-descent</id><content type="html" xml:base="http://localhost:4000/gradient-descent">&lt;h2 id=&quot;giới-thiệu&quot;&gt;Giới thiệu&lt;/h2&gt;

&lt;p&gt;Chuyện là bây giờ mình sắp kết thúc năm ba rồi và đang trong quá trình chuẩn bị thực tập. Nhưng mà portfolio của mình yếu quá nên giờ mình sẽ quay trở lại viết blog tiếp. Rất may mắn là mình kiếm được một chủ đề không mấy là mới nhưng lại khá là quan trọng trong quá trình học của mình. Mặc dù trường mình có dạy phương pháp này nhưng giáo viên dạy rất là khô khan và lan man (đây cũng là động lực chính để mình viết blog này). Mình mong bài viết này tuy ngắn nhưng sẽ bao quát hết được ý tưởng đằng sau của thuật toán và các khảo sát lí thuyết của thuật toán về tốc độ hội tụ.&lt;/p&gt;

&lt;p&gt;Thuật toán Gradient Descent lần đầu tiên được tìm ra bởi nhà toán học nổi tiếng Cauchy (cũng là nhân vật ám ảnh với các bạn học sinh cấp 2 với những bài toán bất đẳng thức khó). Sau đó phương pháp này được nghiên cứu bởi Haskell Cury (cha đẻ của ngôn ngữ lập trình Haskell, đi đầu về phong cách lập trình hướng hàm (functional programming)) và từ đó, Gradient Descent khá nổi tiếng trong giới khoa học dưới cái tên steepest descent.&lt;/p&gt;

&lt;p&gt;Bài blog này sẽ được chia làm ba phần với mức độ khó tăng dần: phần đầu mình sẽ giới thiệu về ý tưởng đằng sau thuật toán Gradient Descent, phần hai mình sẽ chứng minh tại sao thuật toán Gradient Descent có thể hoạt động được, và phần cuối sẽ khảo sát về tốc độ hội tụ của thuật toán.&lt;/p&gt;

&lt;h2 id=&quot;ý-tưởng-của-thuật-toán&quot;&gt;Ý tưởng của thuật toán&lt;/h2&gt;

&lt;p&gt;Hồi xưa khi còn học cấp 3, mình được các thầy cô giảng về cách tìm cực trị bằng việc sử dụng đạo hàm. Ví dụ như tìm giá trị nhỏ nhất của hàm số $y=x^2$ trên tập số thực chẳng hạn. Bước đầu tiên cần làm đó chính là tính đạo hàm của nó, đó là $y’=2x$. Rồi sau đó giải phương trình $y’=0$ để tìm ra các giá trị cực trị x (hàm số này có 1 điểm cực trị là x=0). Tiếp đó, bạn phải khảo sát hàm số trên các khoảng được chia bởi các điểm cực trị (trong trưởng hợp này là $(-\infty, 0)$ và $(0, \infty)$). Cuối cùng, sau khi khảo sát xong bạn kết luận $y$ đạt GTNN khi $x=0$. Cách làm này khá hợp lí nhưng có nhiều bước quá dư thừa: không nhất thiết phải khảo sát hàm số thì mới tìm ra được GTNN (tại sao phải khảo sát toàn bộ hàm số chỉ để giải phương trình $y’=0$ cho ra kết quả?). Nếu bạn là một học sinh Chuyên toán (hoặc trường bạn không giảm tải chương này) thì bạn sẽ được biết đến một cách giải ngắn hơn hiệu quả hơn dựa trên một định lí đề cập Sách giáo khoa:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Định lí 1:&lt;/strong&gt; Cho một hàm số $f(x)$ liên tục trên $\mathbb{R}$.&lt;/p&gt;

  &lt;p&gt;Khi đó, điểm $x_0$ được gọi là:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;điểm cực tiểu nếu $f’(x_0) = 0$ và $f”(x_0) &amp;gt; 0$.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;điểm cực đại nếu $f’(x_0)=0$ và $f”(x_0) &amp;lt; 0$.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Định lí này giúp cho việc tìm cực tiểu hoặc cực đại của hàm số trở nên dễ dàng hơn không chỉ về mặt toán học mà còn về mặt lập trình (trên thực tế, các thuật toán tối ưu hoá trong lĩnh vực Khoa học Máy tính đều cố gắng chuyển hoá các bài toán cực trị thành các bài toán giải phương trình và hệ phương trình rồi sử dụng các phương pháp lặp để tìm ra nghiệm của bài toán). Định lí 1 là tiền đề cho phương pháp Newton-Raphson trong việc tìm giá trị cực tiểu (hoặc cực đại) của một hàm số lồi (hoặc lõm). Ý tưởng rất đơn giản: giả sử $f$ đã được chứng minh là một hàm lồi (hoặc lõm), điểm tối ưu của $f$ chính là nghiệm của phương trình $f’(x) = 0$ và cũng là điểm hội tụ của dãy số $(x_n)$ xác định bởi:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left\{\begin{array}{l}
        x_0\\
        x_{n + 1} = x_n - \dfrac{f'(x_n)}{f''(x_n)}\forall n\geq 0
    \end{array}\right.&lt;/script&gt;

&lt;p&gt;Phương pháp Newton-Raphson hiệu quả về mặt lí thuyết và có nhiều ứng dụng trong thực tế (như chức năng giải nghiệm trên các máy tính cầm tay có mặt trên thị trường). Tuy nhiên, khi nói đến khía cạnh ứng dụng, phương pháp này gặp phải 2 trở ngại lớn:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Phương pháp này đòi hỏi việc chọn $x_0$ rất gần so với nghiệm thực tế của bài toán, nếu không dãy số thiết lập rất dễ bị phân kì.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Trong các bài toán hàm số đa biến, việc này đòi hỏi phải tính nghịch đảo của ma trận Hessian. Tính đến hiện tại, các thuật toán tính nghịch đảo của ma trận đòi hỏi độ phức tạp thời gian và không gian rất lớn. Do đó, tuy tốc độ hội tụ của phương pháp Newton-Raphson nhanh, bộ nhớ và thời gian thực thi của phương pháp có thể sẽ hết trước khi bài toán hội tụ.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Có thuật toán nào chỉ cần tìm ra đạo hàm bậc 1 có thể suy ra được nghiệm tối ưu của hàm $f$ (nếu $f$ là hàm lồi hoặc lõm)? Đó là ý tưởng chính của thuật toán Gradient Descent. Phát biểu của thuật toán rất đơn giản như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cho hàm số lồi $f$ và dãy số $(x_n)$ được định nghĩa bởi:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left\{\begin{array}{l}
        x_0\\
        x_{n + 1} = x_n - \alpha_n * f'(x)\forall n\geq 0
    \end{array}\right.&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Khi đó tồn tại dãy số không âm $(\alpha_n)$ để dãy số $(x_n)$ hội tụ về điểm cực tiểu.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Về chứng minh toán học, mình sẽ đề cập ở phần sau. Ở phần này, mình muốn cho các bạn thấy ý tưởng đằng sau thuật toán này. Ý tưởng xuất phát từ một quan sát rất tầm thường với các bạn học sinh cấp 3: nếu f giảm trên đoạn $[a, b]$ thì $f’(x)\leq 0$ và ngược lại nếu $f$ tăng trên đoạn $[a, b]$ thì $f’(x)\geq 0$. Vậy nếu chọn đại một điểm $(x_0, f(x_0))$ và $x_0$ nằm bên trái điểm cực tiểu thì $f’(x_0) &amp;lt; 0$ (giả sử f là hàm lồi) nên $x_1$ sẽ di chuyển về bên phải, tức là hướng về điểm cực tiểu (Minh hoạ bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/0*En4lt8S2kEwtSkjV.gif&quot; alt=&quot;gradient_descent_left&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ở hình này, người minh hoạ đang chỉnh &lt;span class=&quot;nv&quot;&gt;alpha_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.1 với mọi n &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;dòng đầu tiên&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

và chạy từng bước của thuật toán gradient descent. Như các bạn thấy, &lt;span class=&quot;k&quot;&gt;do

&lt;/span&gt;x0 nằm bên tay trái điểm cực tiểu nên x1 sẽ được cộng một lượng dương từ x0

và di chuyển về bên phải, tiến về điểm cực tiểu.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lí luận tương tự, nếu $x_0$ nằm bên tay phải của điểm cực tiểu thì $f’(x_0) &amp;gt; 0$ và $x_1$ sẽ đi về bên trái, cũng hướng về điểm cực tiểu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/864/1*8HJvJ1bmPukRvbWZaMt-bQ.gif&quot; alt=&quot;gradient_descent_right&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ở hình này, người minh hoạ đang chỉnh &lt;span class=&quot;nv&quot;&gt;alpha_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.01 với mọi n &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;dòng đầu tiên&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

và chạy từng bước của thuật toán gradient descent. Như các bạn thấy, &lt;span class=&quot;k&quot;&gt;do

&lt;/span&gt;x0 nằm bên tay phải điểm cực tiểu nên x1 sẽ bị trừ đi một lượng từ x0

và di chuyển về bên trái, tiến về điểm cực tiểu.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Nếu bạn tinh ý, bạn sẽ thắc mắc liệu $x_n$ có thể di chuyển quá điểm cực tiểu không? Câu trả lời là có nếu bạn chọn dãy $(\alpha_n)$ không phú hợp ($\alpha_n$ quá lớn với mọi n). Tưởng tượng dãy $(\alpha_n)$ không phù hợp giống như bạn chơi cầu trượt khi chạm đến đáy có bôi nhớt bạn sẽ phải trượt ra thêm một đoạn nữa (và té dập mặt) thì trường hợp này cũng giống như vậy. Chính vì điều này mà $(\alpha_n)$ có một tên gọi khác là tốc độ học (tên tiếng Anh là learning rate). Hình phía dưới minh hoạ cho việc gradient descent thất bại khi chọn tốc độ học quá lớn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/1*Q-2Wh0Xcy6fsGkbPFJvMhQ.gif&quot; alt=&quot;gradient_descent_fail&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ở hình này, từ trái qua phải là các trường hợp chọn tốc độ học alpha_n bằng 0.03, 0.4, 1.02.

Nếu chọn 0.03 thì x_n sẽ đi chậm về phía điểm cực tiểu. 

Nếu chọn 0.4 thì x_n sẽ nhanh chóng hội tụ về điểm cực tiểu.

Nếu chọn 1.02 thì x_n sẽ lạng lách và đi càng ngày càng xa điểm cực tiểu.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Vậy làm sao để chọn tốc độ học phù hợp? Đây là một câu hỏi mở cho thuật toán Gradient Descent. Thông thường, tốc độ học sẽ nằm vào trong khoảng $(0, 1)$ và thường được cố định bằng 1 hằng số qua các vòng lặp. Tuy nhiên, cách này không hiệu quả cho các ứng dụng lớn có dữ liệu nhiều chiều. Chính vì thế, đã có rất nhiều thuật toán giải quyết vấn đề chọn $(\alpha_n)$ bằng việc xây dựng dãy $(\alpha_n)$ thích nghi với từng vòng lặp. Nhưng đó sẽ là chủ đề cho các bài viết sau. Trước khi bước vào phần khảo sát về tính hội tụ của thuật toán, mình xin để phiên bản Gradient Descent cho các hàm số đa biến:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Cho $f:\mathbb{R^n}\to\mathbb{R}$ là một hàm lồi theo mọi biến và dãy vector $(x_n)$ được định nghĩa bởi:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\left\{\begin{array}{l}
        x_0\\
        x_{n + 1} = x_n - \alpha_n * \nabla f\forall n\geq 0
    \end{array}\right.&lt;/script&gt;

  &lt;p&gt;Khi đó tồn tại dãy số không âm $(\alpha_n)$ để dãy vector $(x_n)$ hội tụ về điểm cực tiểu.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;chứng-minh-sự-hội-tụ-của-thuật-toán&quot;&gt;Chứng minh sự hội tụ của thuật toán&lt;/h2&gt;

&lt;p&gt;Trước hết, chúng ta cần phải chứng minh dãy số xây dựng bởi thuật toán trên hội tụ với dãy $(\alpha_n)$ thích hợp. Tất cả các chứng minh kế tiếp đây của mình sẽ thực hiện trên hàm 1 biến nhưng có thể mở rộng ra hàm nhiều biến. Mở rộng thế nào sẽ là phần của các bạn.&lt;/p&gt;

&lt;p&gt;Để có thể chứng minh sự hội tụ của thuật toán Gradient Descent, chúng ta cần đến một định lí tổng quát hơn có tên là định lí Bolzano-Weierstrass. Định lí được phát biểu như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nếu dãy $(x_n)$ đơn điệu tăng và bị chặn trên hoặc đơn điệu giảm và bị chặn dưới thì dãy $(x_n)$ hội tụ.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Chứng minh&lt;/em&gt;&lt;/strong&gt;: Mình sẽ chứng minh với trường hợp dãy đơn điệu tăng. Chứng minh tương tự với dãy đơn điệu giảm.&lt;/p&gt;

  &lt;p&gt;Do $(x_n)$ là một dãy bị chặn trên nên dãy $(x_n)$ có chặn trên đúng (chặn trên nhỏ nhất). Kí hiệu là $c$. Như vậy, với mọi số $\epsilon &amp;gt; 0$, phải tồn tại một số $n_0$ nào đó thoả $x_{n_0} &amp;gt; c - \epsilon$ (nếu không $c - epsilon$ sẽ là chặn trên đúng của dãy, vô lí). Do $(x_n)$ là một dãy tăng nên ta có thể suy ra được:&lt;/p&gt;

  &lt;p&gt;với mọi $\epsilon &amp;gt; 0$, tồn tại $n_0$, sao cho với mọi $n &amp;gt; n_0$: $c - \epsilon &amp;lt; x_{n_0} &amp;lt; x_n &amp;lt; c &amp;lt; c + \epsilon\Leftrightarrow |x_n - c| &amp;lt; \epsilon$.&lt;/p&gt;

  &lt;p&gt;Suy ra được $\lim_{n\to\infty} x_n = c$. Do đó ta có điều phải chứng minh.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Quay về bài toán chính, dễ thấy $x_{n+1} - x_{n} = -\alpha_n f’(x_n)$. Nếu $x_0$ nhỏ hơn điểm cực trị và ta xây dựng dãy $(\alpha_n)$ sao cho $\alpha_n &amp;lt; \frac{x_n - x^*}{f’(x_n)}$ với $x^*$ là điểm cực trị thì dãy $(x_n)$ sẽ trở thành 1 dãy tăng và bị chặn trên bởi $x^*$. Như vậy $(x_n)$ sẽ hội tụ về điểm cực tiểu. Trường hợp $x_0$ lớn hơn điểm cực tiểu với cách tương tự ta cũng có thể thu được dãy $(x_n)$ hội tụ. Vậy suy ra điều phải chứng minh. Như vậy, có thể thấy với dãy $(\alpha_n)$ phù hợp, thuật toán Gradient Descent sẽ đảm bảo sự hội tụ về điểm tối ưu.&lt;/p&gt;

&lt;h2 id=&quot;tốc-độ-hội-tụ-của-gradient-descent&quot;&gt;Tốc độ hội tụ của Gradient Descent&lt;/h2&gt;

&lt;p&gt;Một trong những cách đánh giá tốc độ hội tụ của một dãy số là sử dụng kí hiệu $\mathcal{O}$:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Một dãy số $(x_n)$ hội tụ đến $L$ với tốc độ $\mathcal{O}(f(n))$ nếu và chỉ nếu $|x_n - L| = \mathcal{O}(f(n))$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Chúng ta giả sử hàm $f$ là một hàm đa biến lồi và liên tục L-Lipschitz (tức là $||\nabla f(y) - \nabla f(x)||\leq L ||y-x||$, cần phải giả sử đây là hàm L-Lipschitz để vector gradient không thay đổi đột ngột trong quá trình thực thi). Như vậy, tốc độ hội tụ của thuật toán Gradient Descent là $\mathcal{O}\left(\dfrac{1}{n}\right)$ nếu chọn $\alpha_n = \alpha &amp;lt; \dfrac{1}{L}$ với n là số lần chạy thuật toán Gradient Descent.&lt;/p&gt;

&lt;p&gt;Để chứng minh được điều này, chúng ta cần chứng minh bất đẳng thức sau:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_n) - f(x^*)\leq\dfrac{\|x_0-x^*\|}{2n\alpha}&lt;/script&gt;

&lt;p&gt;với $x^*$ là điểm cự tiểu, $n$ là số vòng lần thực thi Gradient Descent, và $\alpha$ là tốc độ học.&lt;/p&gt;

&lt;p&gt;Do $f$ là một hàm L-Lipschitz nên&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_n)\leq f(x_{n-1})+\nabla f(x_{n-1})^T(x_n-x_{n-1})+\dfrac{L}{2}\|x_{n}-x_{n-1}\|_2^2&lt;/script&gt;

&lt;p&gt;Mà $x_n - x_{n-1} = -\alpha\nabla f(x_{n-1})$ nên&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
        f(x_n)&amp;\leq f(x_{n-1}) + \alpha\|\nabla f(x_{n-1})\|_2^2 + \dfrac{L}{2}\alpha^2\|\nabla f(x_{n-1})\|_2^2\nonumber\\
        &amp;=f(x_{n-1}) + \alpha\|\nabla f(x_{n-1})\|_2^2\left(1 - \dfrac{L}{2}\alpha\right)\nonumber\\
        &amp;\leq f(x_{n-1}) - \dfrac{\alpha}{2}\|\nabla f(x_{n-1})\|_2^2\label{eq:1}
    \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Do $f$ là một hàm đa biến lồi nên&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
        f(x*)\geq f(x_{n-1})+\nabla f(x_{n-1})^T(x^* - x_{n-1})\nonumber\\
        \Leftrightarrow f(x_{n-1})\leq f(x^*)+\nabla f(x_{n-1})^T(x_{n-1} - x^*)\label{eq:2}
    \end{align}&lt;/script&gt;

&lt;p&gt;Thay $(\ref{eq:2})$ vào $(\ref{eq:1})$, ta có:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
        f(x_n) &amp;\leq f(x^*)+\nabla f(x_{n-1})^T(x_{n-1} - x^*) - \dfrac{\alpha}{2}\|\nabla f(x_{n-1})\|_2^2\nonumber\\
        &amp;=f(x^*) + \dfrac{1}{2\alpha}\left(2\alpha\nabla f(x_{n-1})^T(x_{n-1} - x^*) - \alpha^2\|\nabla f(x_{n-1})\|_2^2\right)\nonumber\\
        &amp;=f(x^*) + \dfrac{1}{2\alpha}(\|x_{n-1} - x^*\|_2^2 - \|x_{n-1} - x^* - \alpha\nabla f(x_{n-1})\|_2^2)\nonumber\\
        &amp;=f(x^*) + \dfrac{1}{2\alpha}(\|x_{n-1} - x^*\|_2^2 - \|x_n - x^*\|_2^2)\nonumber\\
        \Leftrightarrow &amp;f(x_n) - f(x^*)\leq\dfrac{1}{2\alpha}(\|x_{n-1} - x^*\|_2^2 - \|x_n - x^*\|_2^2)\nonumber
    \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Từ đây, ta có được&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^nf(x_i) - f(x^*)\leq\dfrac{1}{2\alpha}(\|x_0 - x^*\|_2^2 - \|x_n - x^*\|_2^2)&lt;/script&gt;

&lt;p&gt;Vì $f(x_i)$ giảm khi i tăng nên $n(f(x_n) - f(x^*))\leq\dfrac{1}{2\alpha}(||x_0 - x^*||_2^2 - ||x_n - x^*||_2^2)\leq\dfrac{1}{2\alpha}||x_0 - x^*||_2^2$ hay $f(x_n) - f(x^*)\leq\dfrac{||x_0 - x^*||_2^2}{2n\alpha}$. Từ đây, ta có được điều phải chứng minh. Như vậy, trong trường hợp trung bình, thuật toán Gradient Descent có tốc độ hội tụ $\mathcal{O}\left(\dfrac{1}{n}\right)$ với $\alpha\leq\dfrac{1}{L}$. Tuy tốc độ này không nhanh bằng việc sử dụng phương pháp Newton-Raphson (hội tụ bậc 2), thuật toán Gradient Descent ổn định hơn do chỉ tính đạo hàm một lần và vẫn đảm bảo tốc độ hội tụ ở ngưỡng chấp nhận được.&lt;/p&gt;

&lt;h2 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h2&gt;

&lt;p&gt;Thuật toán Gradient Descent nhìn chung là một thuật toán ổn định với sự đảm bảo trong việc tìm ra nghiệm tối ưu sau một số lượng vòng lặp đủ lớn. Ý tưởng của thuật toán Gradient Descent cũng rất đơn giản trực tiếp nhưng vẫn có thể đánh bại các thuật toán tối ưu hoá lâu đời. Điều đáng tiếc là thuật toán Gradient Descent có tốc độ hội tụ chậm hơn các phương pháp lâu đời khác và việc phải lựa chọn tốc độ học sao cho phù hợp đôi khi đòi hỏi sự dày công thử nghiệm (việc tính hệ số Lipschitz cũng đòi hỏi độ phức tạp khá lớn). Chính vì vậy, nhiều phiên bản cải tiến hơn của Gradient Descent đã ra đời nhưng đó là chủ đề cho các bài viết tiếp theo.&lt;/p&gt;</content><author><name>Pham Hoang Minh</name></author><category term="maths" /><category term="optimization" /><category term="convergence-analysis" /><summary type="html">Giới thiệu</summary></entry></feed>