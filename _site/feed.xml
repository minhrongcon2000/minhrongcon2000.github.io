<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-08-21T20:19:42+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Wi-tech’s Blog</title><subtitle>A blog post about technical stuff, especially machine learning by an IT student from International University</subtitle><entry><title type="html">RL Series - Các giải thuật trong bài toán không biết trước được mô hình tương tác giữa các trạng thái của môi trường</title><link href="http://localhost:4000/sarsa-q-learning" rel="alternate" type="text/html" title="RL Series - Các giải thuật trong bài toán không biết trước được mô hình tương tác giữa các trạng thái của môi trường" /><published>2021-08-21T00:00:00+07:00</published><updated>2021-08-21T00:00:00+07:00</updated><id>http://localhost:4000/sarsa-q-learning</id><content type="html" xml:base="http://localhost:4000/sarsa-q-learning">&lt;h2 id=&quot;1-giới-thiệu&quot;&gt;1. Giới thiệu&lt;/h2&gt;

&lt;p&gt;Trong bài viết &lt;a href=&quot;./pi-vi&quot;&gt;trước&lt;/a&gt;, mình đã giới thiệu về hai thuật toán Policy Iteration và thuật toán Value Iteration trong việc giải các bài toán mà các đặc tính của môi trường đã được biết sẵn (cụ thể là mô hình Markov của môi trường đã được tìm ra từ trước khi thuật toán được thực thi). Trong bài viết này, mình sẽ viết về trường hợp mà bạn không biết được hết toàn bộ đặc tính của môi trường và cách hiệu quả nhất để có thể thu thập dữ liệu đó chính là tương tác và cải thiện policy dựa trên phản hồi.&lt;/p&gt;

&lt;h2 id=&quot;2-phát-biểu-bài-toán&quot;&gt;2. Phát biểu bài toán&lt;/h2&gt;

&lt;p&gt;Bài toán RL trong trường hợp môi trường không được biết trước toàn bộ hiện đang là một trong những bài toán nhức nhối trong giới RL. Bài toán được phát biểu như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cho một môi trường $E$ có $\mathcal{S}$ là tập hợp các state và $\mathcal{A}$ là tập hợp các tương tác có thể giữa agent và $E$. Các state thuộc $\mathcal{S}$ có thể chuyển đổi qua lại lẫn nhau dưới một mô hình xác suất mà agent không hề được biết trước. Tìm một policy $\pi$ sao cho hàm value $V^\pi(s)$ đạt giá trị lớn nhất với mọi $s$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Qua phát biểu của bài toán, có thể thấy cách duy nhất để một agent có thể cải thiện policy của mình đó là thông qua việc tổng hợp các phản hồi của môi trường lại và đưa qua quyết định tốt nhất (hàm $\pi$ cực đại hoá hàm $V^\pi$). Vấn đề lại nằm ở chỗ làm thế nào để từ một lượng lớn phản hồi của môi trường có thể đưa ra policy tốt nhất. Quay lại thuật toán Value Iteration, nếu bạn để ý, việc chọn hành động a sao cho hàm $\pi(s) = \arg\max_a Q^\pi (s, a)$ có khả năng giúp thuật toán hội tụ mặc dù chậm. Nên bài toán trở nên vô cùng đơn giản từ việc tìm ra policy $\pi$ sao cho $V^\pi$ đạt giá trị lớn nhất với mọi s thành bài toán ước lượng $Q^\pi (s, a)$ với mọi cặp $(s, a)$ cho trước policy $\pi$, rồi sau đó lấy action $a$ sao cho $Q^\pi (s, a)$ và gán nó thành giá trị của $\pi (s)$. Tuy nhiên, làm sao để ước lượng hàm $Q^\pi (s, a)$? Nếu bạn là một học sinh/sinh viên có kiến thức nền về Xác suất, chắc các bạn biết đến phương pháp cực nổi tiếng sau.&lt;/p&gt;

&lt;h2 id=&quot;3-phương-pháp-xấp-xỉ-monte-carlo-và-ứng-dụng-trong-việc-giải-các-bài-toán-rl-không-biết-trước-môi-trường&quot;&gt;3. Phương pháp xấp xỉ Monte-Carlo và ứng dụng trong việc giải các bài toán RL không biết trước môi trường&lt;/h2&gt;

&lt;p&gt;Chính xác! Không ai khác ngoài nhà toán học nổi tiếng Monte-Carlo. Cho những ai không biết về phương pháp này, sau đây mình sẽ cho một ví dụ. Giả sử bạn muốn biết rằng đồng xu bạn đang cầm có xác suất ra mặt sấp hay ngửa là bao nhiêu. Các bạn sẽ trả lời $50\%$. Nhưng các nhà Toán học và Vật lí học đã chứng minh đồng xu cân bằng là không tồn tại. Từ đó có thể thấy chắc chắn xác suất không thể nào là $50\%$. Thế làm sao bây giờ nhỉ? Vậy thì cứ tung đồng xu lên thôi! Tung đồng xu N lần rồi tính số lần ra mặt ngửa (gọi nó là $M$). Như vậy, xác suất để đồng xu ra mặt ngửa sẽ xấp xỉ bằng $M/N$ nếu $N$ là một số cực lớn (theo qui luật số lớn). Đây chính là tư tưởng của phương pháp Monte-Carlo: để ước lượng một tham số $\theta$, một trong những cách đơn giản nhanh chóng là lấy thật nhiều mẫu từ phân phối của $\theta$ và tính trung bình mẫu là được xấp xỉ gần bằng với $\theta$.&lt;/p&gt;

&lt;p&gt;Như vậy, nếu áp dụng vào giải bài toán RL không rõ môi trường, chỉ cần chú ý rằng $Q^\pi {s, a} = \mathbb{E}[r_t + \gamma r_{t+1} + \ldots | s_t = s, a_t = a]$. Như vậy thì việc đơn giản nhất là đầu tiên tạo ra một policy ngẫu nhiên rồi lấy thật nhiều chuỗi state, reward và ước lượng $Q$ bằng trung bình mẫu rồi lấy $\arg\max$ là xong. Tư tưởng sẽ được mô tả căn bản bởi mã giả dưới đây:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/sarsa_q/naive-monte-carlo.png&quot; alt=&quot;naive-monte-carlo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lưu ý rằng, dòng 3 của vòng for được rút ra bằng quan sát sau: nếu đặt $\widehat{a}_t = \frac{a_1 + a_2 + \ldots + a_t}{t}$ thì $\widehat{a}_t = \frac{1}{t}a_t + \frac{t-1}{t}\widehat{a}_{t-1} = \widehat{a}_{t-1} + \frac{1}{t}(a_t - \widehat{a}_{t-1})$. Tuy nhiên, có một vấn đề: giả sử agent ở state $s$ và trước mặt là tường, policy ứng với s được khởi tạo là đi về trước. Thế thuật toán đó có cải thiện gì ở policy không? Tất nhiên là không. Đây chính là vấn đề khám phá và khai thác trong các bài toán RL (exploration và exploitation). Nếu agent của bạn không chịu khám phá mà chỉ biết khai thác policy hiện có thì có một vấn đề nghiêm trọng đó là nếu policy hiện có đã tệ rồi thì nó sẽ chẳng cải thiện gì thêm (trường hợp trước, do policy khởi tạo đã là đâm đầu vào tường, lấy mẫu của state đó là một việc vô ích). Vậy nên có một cách có thể tận dụng để có thể cải thiện thuật toán này đó chính là sử dụng thuật toán đó chính là giới thiệu một chút random vào trong thuật toán bắt agent phải khai thác policy cũ với xác suất $1 - \epsilon$ và chọn random bất kì action nào trong $\mathcal{A}$ với xác suất $\epsilon$. Thuật toán này gọi là $\epsilon$-greedy Monte-Carlo. Mã giả của thuật toán được mô tả như hình dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/sarsa_q/e-greedy-mc.png&quot; alt=&quot;e-greedy-mc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thuật toán trên sẽ hội tụ về policy tối ưu nếu và chỉ nếu nó thoả mãn điều kiện GLIE (viết tắt cho Greedy Limit of Infinite Exploration). Điều kiện này phát biểu như sau: một thuật toán RL được gọi là GLIE nếu và chỉ nếu thoả mãn 2 điều kiện sau:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Các cặp $(s, a)$ được đếm vô số lần hay $N(s,a)\to\infty$ khi $i\to\infty$ (i là số vòng lặp của thuật toán).&lt;/li&gt;
  &lt;li&gt;$\lim_{i\to\infty}\pi(a|s) = \arg\max_a Q(s, a)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Một trong những cách đơn giản nhất là xây dựng chuỗi $\epsilon_i=\frac{1}{i}$ với $i$ là số vòng lặp. Qua đó, thuật toán sẽ vừa thăm dò các cặp $(s, a)$ vô số lần (do có yếu tố random) vừa hội tụ được policy (do mức độ khai thác sẽ tăng theo thời gian, còn mức độ khai phá sẽ giảm). Nhìn chung thì thuật toán này đã giải quyết được vấn đề không khai phá của thuật toán Monte-Carlo thuần nhưng nó lại không thoát khỏi việc là một thuật toán random. Trong trường hợp agent đã khám phá một policy là tệ, việc random sẽ không hề đảm bảo rằng agent sẽ không đi lại nước đi đó nữa. Chúng ta cần một thuật toán có thể bằng một cách nào đó tận dụng dữ liệu một cách hiệu quả không chỉ lấy mẫu một cách random.&lt;/p&gt;

&lt;h2 id=&quot;bootstrapping-học-từ-kinh-nghiệm-trong-quá-khứ&quot;&gt;Bootstrapping: Học từ kinh nghiệm trong quá khứ&lt;/h2&gt;

&lt;p&gt;Chúng ta hãy cùng quan sát lại thuật toán Value Iteration một chút&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/value_iteration.png&quot; alt=&quot;value_iteration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Như các bạn có thể thấy, thuật toán Value Iteration hội tụ chính là nhờ vào việc sử dụng toán tử contraction kết hợp với các giá trị value của các iteration trước (một toán tử $O$ được gọi là contraction nếu và chỉ nếu $|O(V’) - O(V)| &amp;lt; |V’ - V|$, nếu các bạn đọc phần Phụ lục của bài viết &lt;a href=&quot;./pi-vi&quot;&gt;trước&lt;/a&gt; sẽ thấy điều này). Nói một cách khác, thuật toán Value Iteration hội tụ là do nó đã sử dụng các giá trị value từ vòng lặp trước để cải thiện hàm value ở vòng lặp sau. Việc sử dụng giá trị phía trước để tính các giá trị đằng sau đó được gọi là bootstrapping. Rõ ràng, bootstrapping cũng đóng vai trò quan trọng trong Policy Iteration (bạn phải bootstrap hàm value trước rồi mới tính được hàm $Q$ sau đó mới cải thiện được policy).&lt;/p&gt;

&lt;p&gt;Như vậy, có cách nào để vừa sampling (như Monte-Carlo) mà vừa bootstrapping (như các thuật toán trong bài viết &lt;a href=&quot;./pi-vi&quot;&gt;trước&lt;/a&gt;) không? Đây chính là tư tưởng chính của hai thuật toán SARSA và Q-Learning nổi tiếng.&lt;/p&gt;

&lt;h3 id=&quot;sarsa---state-action-reward-state-action&quot;&gt;SARSA - State Action Reward State Action&lt;/h3&gt;

&lt;p&gt;Cách đầu tiên là bạn lấy mẫu state sau và bootstrap giá trị $Q^\pi$ của state sau và action ứng với state sau theo policy $\pi$. Mã giả của thuật toán được mô tả ở hình dưới đây:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/sarsa_q/sarsa.png&quot; alt=&quot;sarsa.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So sánh với thuật toán $\epsilon$-greedy Monte-Carlo, thuật toán SARSA đã thay hệ số $\frac{1}{N(s,a)}$ bằng $\alpha$ và thay biến $G$ bằng biểu thức $r_t + \gamma Q^\pi(s_{i+1}, a_{i+1})$. Hằng số $\alpha$ lúc này đã có một cái tên khác cực kì thân thuộc với các thuật toán khác trong họ Máy học, tốc độ học hay learning rate. Và rõ ràng, hành động tính $r_t + \gamma Q^\pi(s_{i+1}, a_{i+1})$ chính là hành động bootstrap giá trị $Q$ của cặp $(s, a)$ kế tiếp theo policy $\pi$. Việc chứng minh trực tiếp hội tụ của thuật toán này hiện chưa có (theo bài báo &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.235&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;này&lt;/a&gt;) nhưng nó được công nhận không chứng minh rằng nếu thuật toán SARSA thoả GLIE và tốc độ học $\alpha_i$ thoả mãn điều kiện Robbins-Munro ($\sum_{i=1}^\infty\alpha_i = \infty$ và $\sum_{i=1}^\infty\alpha_i^2 &amp;lt; \infty$) thì thuật toán hội tụ (dãy $\alpha_i = \frac{1}{i}$ là một dãy thoả mãn điều kiện này). Có một điều mình muốn nhắc (vì đây là khái niệm mình mới bị cô hỏi mà lóng ngóng) đó là hai khái niệm thuật toán on-policy và thuật toán off-policy. Các thuật toán policy luôn khởi tạo một policy ban đầu và cố gắng cải thiện policy bằng cách đi theo thuật toán đó. Các thuật toán off-policy lại cố gắng cải thiện policy của mình bằng cách đề xuất ra một policy khác, so sánh giữa policy hiện tại và policy thay thế và thay đổi policy tuỳ vào ngữ cảnh của agent. Một ví dụ của thuật toán on-policy chính là thuật toán SARSA. Thuật toán SARSA học bằng cách men theo “hướng” của policy đầu để có thể cải thiện policy của mình. Chính vì vậy, việc khởi tạo policy là một việc cực kì nhức đầu với thuật toán này vì nếu khởi tạo sai thì bạn sẽ hội tụ đến policy rất là tệ (có nhiều dao động và thậm chí là dao động mạnh). Tuy nhiên, theo định nghĩa, các bạn có thể thấy Value Iteration và Policy Iteration là hai thuật toán off-policy. Vậy liệu có thuật toán off-policy nào vừa sampling vừa bootstrapping không? Câu trả lời nằm ở phần dưới đây.&lt;/p&gt;

&lt;h3 id=&quot;thuật-toán-q-learning&quot;&gt;Thuật toán Q-Learning&lt;/h3&gt;

&lt;p&gt;Thuật toán Q-Learning kế thừa thuật toán SARSA nhưng có một chút biến đổi: chiến thuật của nó lần này là bootstrap hành động có giá trị $Q$ lớn nhất thay vì chỉ bootstrap hành động ứng với state kế tiếp. Mã giả của thuật toán được mô tả như hình dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/sarsa_q/q-learning.png&quot; alt=&quot;q-learning.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Như mình đã đề cập, thuật toán này bootstrap hành động có giá trị $Q$ lớn nhất bằng cách sử dụng biểu thức $r_t + \gamma \max_a Q^\pi (s_{t+1},a)$. Điều này cũng có nghĩa thuật toán Q-Learning sẽ có một policy thay thế (vì hành động của state kế tiếp có khả năng sẽ bị thay đổi do lấy max). Do đó, Q-Learning là một thuật toán off-policy. Thuật toán Q-Learning cũng có tốc độ hội tụ phụ thuộc vào việc khởi tạo policy ban đầu nhưng nó không quá nặng bằng SARSA do Q-Learning có khả năng đề ra policy thay thế để cải thiện mô hình cũ. Thuật toán Q-Learning có hai mức độ hội tụ. Nếu chỉ quan tâm đến việc tìm được giá trị $Q$ tối ưu thì chỉ cần thuật toán phải đi qua mỗi cặp $(s, a)$ vô số lần và $\alpha_t$ phải thoả điều kiện Robbins-Munro. Tuy nhiên, nếu muốn hội tụ tối ưu thì thuật toán này phải là GLIE và điều kiện Robbins-Munro phải được thoả mãn.&lt;/p&gt;

&lt;h2 id=&quot;thí-nghiệm-thực-tế-trên-hai-thuật-toán&quot;&gt;Thí nghiệm thực tế trên hai thuật toán&lt;/h2&gt;

&lt;p&gt;Vâng, đây là quá trình khổ sở nhất các bạn ạ (vì việc khởi tạo ảnh hưởng lớn đến độ hội tụ của hai thuật toán nên mình phải chạy đi chạy lại khá nhiều lần, mỗi lần 8 tiếng). Trước khi bước vào kết quả, mình xin mô tả môi trường của mình một chút. Lần này mình sử dụng môi trường mê cung ở &lt;a href=&quot;https://github.com/MattChanTK/gym-maze/&quot;&gt;đây&lt;/a&gt;. Môi trường có dạng như sau:&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;./assets/sarsa_q/env.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt; 
&lt;/center&gt;

&lt;p&gt;Trong hình trên, ô vuông xanh ở góc trên bên trái mê cung là ô vuông bắt đầu, ô vuông bên dưới góc phải màu đỏ là ô kết thúc, chấm tròn màu xanh chính là agent, các cạnh màu đen đậm là tường. Tại vị trí $(i, j)$, khi agent thực hiện action $a$ có hai khả năng xảy ra: nếu hành động $a$ không đụng tường thì hành động $a$ được thực thi, vị trí mới và reward mới được trả về; nếu hành động $a$ đụng tường, vị trí $(i, j)$ được trả về và reward hiện tại ở vị trí $(i, j)$ sẽ được trả về. Hàm reward của mình sẽ được định nghĩa là $-\frac{0.01}{n}$ với $n$ là số step đã đi được. Điều này sẽ làm cản trở việc agent đi đường cực dài để đến ô đỏ. Ngoài ra việc sử dụng phép chia sẽ làm cản trở việc agent chỉ đi chung quanh vị trí bắt đầu mà vẫn có reward.&lt;/p&gt;

&lt;p&gt;Và đây là kết quả của mình:&lt;/p&gt;

&lt;p&gt;Thuật toán SARSA:&lt;/p&gt;

&lt;iframe width=&quot;891&quot; height=&quot;501&quot; src=&quot;https://www.youtube.com/embed/oruyR6-R0OM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Thuật toán SARSA mình gặp rất nhiều vận may vì chỉ cần 2 lần chạy mình đã có agent hội tụ chỉ sau 125 vòng lặp (như trên video mình up trên youtube). Điều thú vị là thuật toán này lại bỏ đi việc khai phá rất nhanh và tập trung vào khai thác policy hiện hành (chắc do mình ăn hên được policy đầu tốt).&lt;/p&gt;

&lt;p&gt;Thuật toán Q-Learning:&lt;/p&gt;

&lt;iframe width=&quot;891&quot; height=&quot;501&quot; src=&quot;https://www.youtube.com/embed/KPwNsHukWtc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Thuật toán Q-Learning mình gặp xui nhiều nhất. Mình đã chạy tổng cộng 5 lần nhưng chỉ có đúng một lần là thuật toán này hội tụ sau 5000 vòng lặp với performance xấp xỉ bằng performance của SARSA (vì ở cuối Q-Learning agent tốn 3s để đi từ xanh đến đỏ còn SARSA chỉ tốn 2s).&lt;/p&gt;

&lt;h2 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h2&gt;

&lt;p&gt;Vậy khi nào bạn nên sử dụng thuật toán nào? Theo mình đọc trên mạng, thuật toán SARSA được sử dụng khi bạn quan tâm đến performance của agent. Ví dụ trong trường hợp bạn làm một con robot vệ sinh nhà cửa, chẳng may nó té cầu thang thì phải mua con robot khác train tiếp (và một con robot khá là đắt). Việc sử dụng thuật toán SARSA có khả năng sẽ giúp bạn hạn chế điều này vì robot chỉ đi theo policy được khởi tạo ban đầu và dần cải thiện. Thuật toán Q-Learning sử dụng khi bạn chả cần quan tâm đến agent hoạt động thế nào mà chỉ muốn có được policy tối ưu, thường được thấy rất nhiều trong việc train AI để chơi game. Trong bài viết tới, mình sẽ cố gắng viết về việc sử dụng Deep Learning trong các bài toán không biết trước được môi trường.&lt;/p&gt;</content><author><name>Pham Hoang Minh</name></author><category term="maths" /><category term="reinforcement-learning" /><category term="sarsa" /><category term="q-learning" /><summary type="html">1. Giới thiệu</summary></entry><entry><title type="html">RL Series - Thuật toán Policy Iteration và Value Iteration trong giải quyết bài toán tìm đường đi trong ma trận vuông.</title><link href="http://localhost:4000/pi-vi" rel="alternate" type="text/html" title="RL Series - Thuật toán Policy Iteration và Value Iteration trong giải quyết bài toán tìm đường đi trong ma trận vuông." /><published>2021-08-14T00:00:00+07:00</published><updated>2021-08-14T00:00:00+07:00</updated><id>http://localhost:4000/pi-vi</id><content type="html" xml:base="http://localhost:4000/pi-vi">&lt;h2 id=&quot;1-giới-thiệu&quot;&gt;1. Giới thiệu&lt;/h2&gt;

&lt;p&gt;Lần viết blog này (và có thể là những lần viết sắp tới), mình sẽ viết về vấn đề mà mình chuẩn bị phải làm luận văn về nó để tốt nghiệp (năm cuối rồi các bạn, thật là khổ). Thế nên, được sự góp ý của giáo viên hướng dẫn của mình, mình sẽ viết blog về các thuật toán học tăng cường (&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;) rồi kèm với đó là tìm kiếm chủ đề nghiên cứu phù hợp với mình. Mục tiêu của bài viết này sẽ cho các bạn một cái nhìn tổng quan nhất về các bài toán học tăng cường và giới thiệu 2 thuật toán cơ bản nhất trong lĩnh vực này là &lt;strong&gt;Policy Iteration&lt;/strong&gt; và &lt;strong&gt;Value Iteration&lt;/strong&gt;. Trong bài viết này, nhiều thuật ngữ chuyên ngành mình xin được phép không sử dụng từ Tiếng Việt vì hầu hết các bản dịch tiếng Việt đều không ổn và không diễn tả hết nghĩa của thuật toán.&lt;/p&gt;

&lt;h2 id=&quot;2-ý-tưởng-đằng-sau-bài-toán-reinforcement-learning&quot;&gt;2. Ý tưởng đằng sau bài toán Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Như các bạn đã biết, máy học hay machine learning là một lĩnh vực nhỏ của trí tuệ nhân tạo sử dụng các mô hình để tìm ra giải pháp tổng quát nhất cho 1 vấn đề nào đó dựa trên kinh nghiệm có sẵn. Ví dụ như các mạng tích chập cố gắng xây dựng một mô hình tổng quát để gán nhãn cho ảnh dựa trên một tập dự liệu ảnh đã gán nhãn sẵn (1), hay thuật toán K-Means xây dựng một mô hình để nhóm các thông tin khách hàng và từ đó có thể phân khúc thụ trường (2). Hai ví dụ trên cũng là hai ví dụ điển hình của hai loại bài toán phổ biến nhất trong lĩnh vực máy học: học có giám sát (ví dụ 1) và học không giám sát (ví dụ 2). Với các bài toán học có giám sát, điều tiên quyết là cần một lượng dữ liệu lớn đã dán nhãn và mục tiêu là xây dựng mô hình để tổng quát hoá mối quan hệ giữa dữ liệu và nhãn. Học không có giám sát thì ngược lại cần 1 lượng dữ liệu lớn nhưng không có nhãn và thường sử dụng các thuật toán để tìm 1 cách nhóm dữ liệu phù hợp để có thể chia dữ liệu ra thành nhiều phần nhỏ tách biệt nhau. Ngoài ra, học không giám sát còn bao gồm cả việc đơn giản hoá dữ liệu huấn luyện để có thể sử dụng vào trong các thuật toán học có giám sát (ví dụ tiêu biểu là hai thuật toán PCA và LDA).&lt;/p&gt;

&lt;p&gt;Tuy nhiên, trong một số trường hợp, việc thu được dữ liệu để huấn luyện trước khi triển khai vào thực tế lại là một điều không thể thực hiện. Ví dụ như trong bài toán tìm đường đi nhanh nhất đi từ điểm A đến điểm B, ngay cả khi bạn biết được một đồ thị đầy đủ giữa hai điểm A và B, bài toán này sẽ trở nên bất khả thi nếu bạn cho thêm các yếu tố ngẫu nhiên như kẹt xe, đèn đỏ, hay các yếu tố thiên tai. Trên các bài giảng lí thuyết, các vấn đề này thường được bỏ qua để đơn giản hoá vấn đề, song, trên thực tế, dưới sự trợ giúp của các công cụ như Google Maps, việc biết trước rằng 1 con đường đang diễn ra ùn tắc là một điều rất dễ trong thời gian ngắn nhưng lại cực kì khó nếu muốn biết thu thập trước và sử dụng nó làm dữ liệu cho quá trình huấn luyện sau này vì dữ liệu đó chỉ giá trị tại một thời điểm nhất định và trở nên vô dụng khi việc ùn tắc đã chấm dứt. Đây chính là lúc mà bài toán Reinforcement Learning được sử dụng để giải quyết vấn đề. Bài toán Reinforcement Learning (RL) nhắm đến việc cải thiện mô hình dựa trên dữ liệu thực tế và tồn tại trong một thời gian ngắn nhưng ảnh hưởng đến toàn bộ 1 quá trình. Trong bài toán tìm đường đi nhanh nhất, một con đường ngắn nhất đi từ A đến B có thể tốn nhiều thời gian nhất vì ai cũng biết nó là con đường ngắn nhất nên ai cũng đổ xô đi con đường đó khiến nó trở nên không còn tối ưu nữa. Vậy một thuật toán RL hiệu quả cần phải biết cách thay đổi con đường sao cho phù hợp với tình cảnh thực tế.&lt;/p&gt;

&lt;p&gt;Trong bài toán RL, luôn luôn có 1 agent và 1 environment. Trong bài toán tìm đường đi, agent chính là người lái xe và environment chính là toàn cảnh những con đường đi từ A đến B kèm với các yếu tố ngẫu nhiên. Các thuật toán RL sẽ cố gắng tìm ra chiến lược (&lt;strong&gt;policy&lt;/strong&gt;) tốt nhất để có thể đi từ A đến B bằng cách cho agent tương tác với environment nhiều lần (ví dụ như lái xe nhiều lần từ A đến B). Sự tương tác của một agent với một environment được mô tả rất đầy đủ trong hình sau:&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWIAAACOCAMAAAA8c/IFAAAAe1BMVEX///8AAAD09PReXl7U1NSrq6u+vr7a2towMDAYGBgpKSn7+/vp6enBwcHR0dHx8fGEhIRmZmY+Pj6xsbHHx8dxcXHf3984ODikpKSZmZlPT08jIyN0dHS3t7eRkZFXV1dHR0dzc3N/f3+Tk5NKSkoLCws0NDQcHByfn59vzkeVAAAOuklEQVR4nO2dCZ+qLBSHAXdBcSHM3bGsvv8nvKDVtFhTM3rLGf+/951rZhSPeDgcNgDeRHjnDS731Zl6K1EfQlgpAyqA0N6hV+frfYQzaNdk2DS5uGvxsElOWeoGrgZPlEcwmovxQRrcjJJqE46Q7CSFMrgbIVm6gd4IyU5StIRj1P7Ih9kIyU5SJIHqCMmiBVyMkOwkNSMeVvj61Ix4WLl17Fy4wDPiYcUsaJlJ4dJPn/WPI6bGwNK2sNOmzl3ecv7jiJk/ZORAqGrgUVGdc4JvI6bkJ63qqSDW4MiqPMZvIKYLM3iMMe5rKc+IOyVFrt5C7Ir3lw/9yl3Jrk9OCHHO+2Sw7yk+2GLYJHFX6d0yFAWMoseCFxnMr09OCPGwVRFLBVyzzHaf5e4GYgzLvIa8PaautmOUu1hWwGEcGlS877rIWeYGwKoCi+sm+B9GvN2UMT89dQMxhx6K95ZCCyC0loU0zbkinoBSlFpSJbqoOxVKK3EmuDLHfxaxobmXMG4gLmAovOhSHuYwXWmLJo0IcGDqaV4DdcAD2yqWEdzhsIIL7arZKBB/DPrTR9LgiHvUjxgHAiioUkMcbwJNnPiAAaGlLct1vKkEYliIo1QBoBbEryQQK0N79Qcxhw6W/dchdpoPgIEnQ8m06xTJ7YhwWHCEEKkhIAE0pAlpEfdXd83WGkcNdAbL/usQezAqy7KCvgtYZ5FZGREDbtPtdivqTEoCi3+BeET9AsRc2FdfyLa8U8QqrLVOwlB8idhcjKRfgZg1a8MV+oC+8C3avmQ9kqU4bOs1TB9BrKjuOPoViPM9NQZtFUhrAcBKVHckLeUhy/xTxLeaHqN5FG+PmNAzv60XcQi78ASphJUoYKDSsBGIQQwDl7obcZIHjazuohbx8jpKMaJfjN8dMa6gsnPco9vTX4q9vaO7s2uEMhvCtFYEYuRForKJPASIUnHZEqmF5wZ7xkz8ZcSglFVyWtZL1mLuRUwOd4AaHBNi5CF3IkW2m3ke5hzLA/mXclHaqTjV1/SYJmIU7uKfaRdv9o7Pxi9EYrrxZUheW+UIGJm16un5u6nJIiabQM/vScs+svv6SD/dS7u+Hcz8FIMwKLdw8dQAqukiDvyfpomTNpZpJwWjslg+0LFE3aLWyDNl+K8jtpIs1u9Xdz/XX0YMGDvrL5oRD4/4QjPiGfGVZsSd0MeM+CCqTG3w6+QQo2yUWRnUHm0I9+QQgxgqg6fZDsMYY/C91PQQs6av7+2HQiVM+deXfUvTQww82Cw4RQOKqsl4hXiKiGkmx0b4A0oA3iyfa3A/oQkiBmipDNx9adUjTtCdImIZ93XCAcXImNMap4lYjmAdUKPZiO6nThTxhDQjHl0z4tE1Ix5dM+LR9UaIMeuZg/EL9EaIUVAO9kveSf8NsfQ98YUDivcnuvdQNUYI7fX6L4hxEcY25oltx0DzDeCUBPgO8II0EF9ea0lB9E1a/1LD/V8QI3+TeFRJvNp2ckHVgwbZGqHlr0oBOwhKRw3KoholEPx63UAsnmDiG/0fOWnQo/P5mjcRlwsXsA3HwPNJ7aESGtoG5BoBjm0Ae4FBUSHg/i3EHxmhq/4YNTKP6xSh/HxM7m3EGgCFWRSFH9Aw4MmCZDVAjuaVWw42OwD8QlwV/XLEyAlzWSZ5HjJAkiTHqiitanuSGEbIsPCqcvHXhTURV+cOBdyTI0qpo+8L833Ea0fXdRer5tJnrGKgiDxnKUrxZgnA4g8gNryNIoeLukFS2ksntTNeuSDflEEi/lmXvpmBFYTQIxWEKywDtgqp5VxBZkNodwX+LuJ8LW5ZWABc2gVQLEoC4QarKW8Re2txc5vfjTjeuNgVRaqsEV5F2Pc5MV1UeYgmEc6hixcQFLmxDAwmSnFuO6K2MgyvMlASMDfpxiDcRUy2HnPMWM61CMGmwrSsVUP22EtDwayV6sPf7RcTg2J9a+BUTmbVwGJBBGLdNGSPL8k3BBQCsU6IgSmMAXEpYamBliVw7ZyQ0G6rxptO20q221gZBIUoyeyDgFp8jZMEZewrRiHfXFZBXYzWffZSHREvy3XVGMjqelg6xLEtiOjQzRMqEYdR5Ocd4tj3hVuAdiUOG9/3laRt+37VukMXnQv0pDXSu0rEr9ABsW/mlEcGbnSRcw13iDVb1GNLyPeIKXeXKZOIi7VruJFEDHLTNQw3bwenzmGgPh0QlxHBO2gAxSfIC8DCl4jROkO8qvAesWZgvtGpqO2yGmMHugIxIqlHkddVVHcQi9J62Xz+Kzogzksz8KPU4IFtWgx4TcnXrjC0diraX7lPhUkGnuWnAacwcpbQ9y3hTWjNChXQDPYz2W4jpkyj8Y2mzG/X0S/Guip8YvG8Mx3JV/sGiX7S++3WC08FWMt0vPvw3DADtI4R1Wtxor3gNmLiKXE1/CCeSeizdXd4lLvn+fBUP/V030aMV8K9++OGYhDdRrwMZKsbjzVs7K11gpiE7vIbDE7K+W3EvqgwAS/+5BLBn4hZRuNvDEA3cuN4X24iJpWESzzt+z90ujoiJjYHq+rpUuwmRnwsmzcRI1UmzLw/6VMcEONCwPC9pxexCVMCjk7HF02PVT3cGjkT0tEvDhygykgDfa4hSyv/8wNfIK6fmzP7W3RAXFgGLgKDqP5TPe0Iq02GAe+K5xeI/2jz7oCYKUm8C3ZUzZ56mF0G1hUg6y4mP8co+vRZ3ekYCTvhPrctiRZzXwNs3ZGdEffppHXX/rd7zq/iTJeLlewDvTPiPl227rLrBUe+VrGPPsyI+3SJmH/Dr8LefsmoGXGf3mhM22/VjHh0zYhH14x4dM2IR9eMeHTNiEfXjHh0zYhH14x4dE0BMWH6C8WMH4a5J4DYiBPzhVpn+s/6w94f8W771YIdo0v5UbfuuyOmCwhLXX2hmNfAzU+20nsV4uWD3+qNuC7So+JrmPzg469CfD5qhbgOzftKCknfYesj126ud2V6WO+BmBcls/u6vh0Lhj2n/7NQCf2pIwZxooV9I5F2sPmJGRxKNdx8fwjImyBelP1ZKKD1DsNbPJhODzE58zXzqO1gpYdZNSTbl12B+B0GaU0S8bkWkQBJYg8Dt905Bin7HM2IL/RtxJVEasACA92TeWHZvnaZEV/oCcRnsxKwJgd3EpuQwl97IcD+wY2YEV/oCcTx9W5dYUaRsVswDmh6aLHOiC/0tEeBjROXzNuJF2wlJ6z6h9Mz4gs9gziUQUqvOdmAnEiuWBLl/JCheEZ8rh7E2771ghVFq0vfr7YQBvdXFo5mxOe6QkzrslfJksn9sIW2pmnflmnNiM91vd8dov3C0kz4JoQrclfeJWJ+VP9PQHyMofjvjPj+d6Pl+osgz+oCMTmGyZv+JEM4wPRJxC+A9iBG5NEvehVi3vkM6AszcOlRELgtlq36h5pjVg0QNnK3F0wuEGM1X2bR+sHUXoW4eCxGeY3YvN/Jg4cIzLHLbS1OERthrbQ7SVYPpvYqxA9OJL2LGBkYMy2U24l2YIWBpgYW/1MQusIm81Dr1iIRz4yad01IDlxNE2mwQ/SU6vsj8RbJNdbmY6eeGYYjYpIXfmV1pqp6MKuv2qT4HDEmjPSajLuI2TYPrAamLsjbyX+umYFc2OLQ3i3kSmdxA5tmKwukUu4scWUMQBbElgXt3Yf4m7YrFm2aprFiDHBRFqa4yEe0gVZztkWCQEwRXy6qk106YRM9qLdAvFLUrHe+3zViO+eGlGwJQstTyQ4qoKvkViIr8kiztsHKFYclM/JNqspdeCuHGGYNgA8TRnMIFZXu5PakvAlClZUwBNiD5o4YPtyhHSycs6/14HaZfLsDu9kMt6XFE4jDM6K+Tfor6GvEB/kScSDPBWukQ03uIdZ0HoW2bdR27y9pPhgsJGIZEtEk4kaWKFtyp0FCBTz5Q0gVYIFY1g+55SO3xxZTVyv86Ixc8Fgh3vxsjMC5nkB8PpNUTS4a2od3+zwKTWrJ9vQAqNaUR4HciarYI25kbJSb3bJv4skH5UamomftzRRHUSINeCIQ24kh3G+aQSoQy7vspAvELreU39tiyt1deUQcEP6Yhuwa+/5W20u5eNkRJl0esthjKFjXnJELvnX9/wKxKLGi/IkCuUecybUorbr9iFkSUEbynvUhhra/EDIhx17zFWLQLnIU+9vnqrsh9U3EVEfAjAFTRL5XbadHcHhQ71d3R8RgB3VXCegBcS2Iqk23iEOQ3EVs1q08ekC8XQhDcRNx+3uJu/I3TfWNrP5YTyBWT0wvyRhRVJB9GIT7KsGAmId3H0XM02wpX50g5lE3vqQ1FDcRpyWVi4FzAzyOWAoj9SWrSX3To0CG64jcLTjWisBbERAeF654FDFQ7LVw3k4R40VrW3W4AncQe23NxisfnxmKO02P1+pHTQ+yIOIR9LkoxZl6u7prDtuwuWeI89bHOEUs4Faas0vlqrTJTcTc2sR6WAqmJ6VY1KM3mh4v13cQY3e/lajb+jZyrQaSuLcRH6UfEAeV/Ay0pHvRIYZt76pwE6K13V6TmEfEW4l4I1d3oGtxb3AM0yqAPgUHj6KRTpuZnGXkrRA/5mTjQ8cSXwXbtDuHiMTQ9nx8BrouI23YOAoJA9O1jlvDbXSzt6kqG9D7GBPz6pX7eQlqV7vG7Wt8OCtusVc48gu7t5D4MNa9+szTeivEdfyQtFhknbtZI4pj6twUq38Ykn8oeNxz0cUCMG+F+FEx4HrB15cFzfR7PQbVE4gdYPgPXFbNiM/FteWDkhFIQHYtxmZxW8qM+Fz4Rk/dtVAuqyFqeLawxbe77uhV391r9D6In9DBaSNL/14HzaVH8SJNGrHQvW7G3zAa6FX6ZsfSizQjHl2TROw+NgphRjy6fsNcjzfX24zMNKeH+MG1+/PmMnT7EilQmd6qtQ9Wd9SGb7CihbNtvrPO5Yv1IGK8gOnLp4+SBQymR/hRxICsISxc44Vy8zW0hhtr8v/08OYI1IMQ3hnlPboaCIPhBkz9R7GHR8hg5r900Q+riv/bNOx/eAF2kU4q6/QAAAAASUVORK5CYII=&quot; /&gt;
    &lt;p&gt;
        &lt;h5&gt;Hình 1: Tương tác giữa agent và environment&lt;/h5&gt;
    &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;Đầu tiên, agent sẽ tương tác với environment bằng cách thực hiện một hành động A. Hành động A này sẽ làm thay đổi trạng thái của environment. Environment sau khi bị thay đổi trạng thái sẽ phản hồi A trạng thái đã thay đổi và một tín hiệu reward (tín hiệu này cho biết hành động vừa rồi là tệ hay hành động vừa rồi là cực kì tốt). Như vậy, có thể dễ dàng thấy được mục tiêu của bài toán RL đó chính là dựa trên các mà environment phản hồi, đề ra một &lt;strong&gt;policy&lt;/strong&gt; tốt nhất để tương tác với môi trường đó. Trong bài toán tìm đường đi ngắn nhất, RL sẽ tìm một agent để có thể tìm đường đi hiệu quả nhất bằng cách cho agent lái đi trên nhiều tuyến đường chứng kiến các yếu tố ngoại cảnh. Đây cũng chính là ý tưởng chính của bài toán RL. Thay vì phải thu thập dữ liệu từ trước như các loại hình học máy khác, RL thu thập dữ liệu trong quá trình thực thi và cải thiện mô hình dựa trên lượng dữ liệu đó.&lt;/p&gt;

&lt;h2 id=&quot;3-phát-biểu-bài-toán-reinforcement-learning&quot;&gt;3. Phát biểu bài toán Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Để có thể phát biểu chính xác bài toán Reinforcement Learning, trước hết chúng ta cần phải định nghĩa chính xác thế nào là environment và agent. Environment có thể hiểu đơn giản là một tập hợp các trạng thái (state) được thay đổi qua lại theo một qui tắc nhất định nào đó ứng với các hành động của agent. Một agent là một thực thể trong environment có khả năng tương tác với environment và thay đổi trạng thái của nó. Agent có thể biết hoặc không biết qui luật đằng sau sự biến đổi giữa các state. Một &lt;strong&gt;policy&lt;/strong&gt; có thể được hiểu là một hàm số theo biến $s$, tức là cứ gặp state $s$ của environment thì agent sẽ thực hiện hành động $a = \pi (s)$. Cũng nên chú ý rằng, hàm $\pi (s)$ này có thể là một hàm phân phối xác suất, không nhất thiết phải là một hàm số có mối quan hệ 1-1 (điều này cực kì hữu hiệu trong các bài toán mà agent không biết qui luật thay đổi state của environment).&lt;/p&gt;

&lt;p&gt;Như vậy thì dường như các bạn đã thấy bài toán này có hao hao cái gì đó để tối ưu, giống như các thuật toán trước. Đó chính là policy $\pi$. Câu hỏi được đặt ra là cái gì sẽ là cái đánh giá độ tốt của một policy. Một ý tưởng cực kì tự nhiên là chỉ cần chọn $\pi$ sao cho tổng reward signal $r_t$ đạt giá trị lớn nhất. Đây là một ý tưởng có lí trên mặt thực tế nhưng về mặt lí thuyết, nó gặp phải một trở ngại: lúc này, tổng reward sẽ không chỉ tỉ lệ thuận theo độ tốt của policy mà còn tỉ lệ thuận với độ dài thời gian mà agent này đã hoạt động. Điều này có nghĩa là một policy khiến cho agent hoạt động vĩnh viễn mà không đến được state mong muốn (và luôn né những state cần tránh) lại trở nên tốt hơn so với policy giúp cho agent đến được state mong muốn chỉ với vài bước (điều này rất dễ diễn ra trong trường hợp có vô hạn state của môi trường). Chính vì vậy, thông thường, policy $\pi$ sẽ được tối ưu thông qua hàm số sau:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\pi}(s) = \mathbb{E}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2}+\ldots | s_t = s]&lt;/script&gt;

&lt;p&gt;với $\gamma$ được gọi là discount factor và hàm $V^{\pi}(s)$ có một tên gọi khác là value function. Dễ thấy, hàm value function đo lường độ tốt của một state dựa theo policy mà agent đang thực hiện. Bằng việc giới thiệu hằng số $\gamma$ như trên, người ta không những giải quyết được vấn đề vô hạn bước mà còn có thể điều khiển mức độ nhìn trước của agent. Nếu $\gamma$ gần với 1 thì những policy dài hạn sẽ cho value function lớn hơn những policy ngắn hạn, từ đó agent sẽ có xu hướng tạo ra những nước đi để lại kết quả dài lâu (điều này phù hợp cho các ứng dụng của RL trong các bài toán tài chính) và ngược lại, nếu $\gamma$ gần với 0 thì policy mà agent có xu hướng thực thi sẽ rất ngắn hạn, tức là chỉ tham lam lấy state liền kề tốt nhất mà không cần để ý đến những bước sau này. Tóm lại, bài toán RL có thể được phát biểu như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cho một biến $s$ là một biến ngẫu nhiên thể hiện state của một environment $E$ và $s$ có thể biến đổi theo một phân phối nhất định ($s$ có thể có hữu hạn hoặc vô hạn giá trị). Biết rằng một agent $T$ chỉ có thể tương tác với $s$ thông qua các hành động trong tập $\mathcal{A}$. Gọi tập hợp các state là $\mathcal{S}$ là tập hợp các state, mục tiêu của bài toán RL là tìm ra một policy $\pi$ là một hàm số đi từ tập $\mathcal{S}$ đến tập $\mathcal{A}$ sao cho hàm $V^{\pi}(s)$ đạt giá trị lớn nhất.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;4-giải-bài-toán-rl-trong-trường-hợp-biết-sẵn-được-mô-hình-tương-tác-giữa-các-state-với-nhau-policy-iteration-và-value-iteration&quot;&gt;4. Giải bài toán RL trong trường hợp biết sẵn được mô hình tương tác giữa các state với nhau: Policy iteration và Value iteration&lt;/h2&gt;

&lt;p&gt;Trước hết chúng ta cần biết mô hình tương tác giữa state với nhau trong environment sẽ có dạng như thế nào. Thông thường các bài toán RL đều phụ thuộc rất nhiều vào các lí thuyết của chuỗi Markov. Một chuỗi Markov có thể hiểu là một chuỗi các state được sắp xếp theo trình tự thời gian sao cho khả năng state sau xuất hiện chỉ phụ thuộc vào việc state phía trước là state gì. Nói một cách toán học, một chuỗi state được gọi là chuỗi Markov nếu và chỉ nếu&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(s_{t+1} = j|s_t=i, s_{t-1}, \ldots ) = P(s_{t+1} = j|s_t=i)&lt;/script&gt;

&lt;p&gt;Việc giả định environment là một chuỗi Markov thường hợp lí trong nhiều trường hợp. Trong bài toán xe tự lái, vị trí tiếp theo của chiếc xe chỉ phụ thuộc vào vị trí hiện tại của cái xe mà không phụ thuộc vào cả quãng đường cái xe đã đi. Trong bài toán chơi cờ chẳng hạn, nước cờ tiếp theo của agent phụ thuộc vào nước cờ của đối phương chứ không phụ thuộc toàn bộ vào cách chơi của đối phương. Tất nhiên, bạn có thể lí luận là có rất nhiều trường hợp mà environment không thể Markov và việc lấy toàn bộ các state phía trước đó. Tuy nhiên thì người ta chứng minh được rằng trong trường hợp thời gian vô hạn thì mọi chuỗi sẽ tiến về chuỗi markov và nếu lấy toàn bộ lịch sử các state thì sẽ gây ra tốn bộ nhớ và không hiệu quả vì trong bộ nhớ sẽ có thể có sự lặp lại của một chuỗi nhiều lần.&lt;/p&gt;

&lt;p&gt;Khi sử dụng chuỗi Markov vào trong các bài toán RL, ngoài việc định nghĩa các state và mô hình xác suất tương tác giữa các state, chúng ta cần phải định nghĩa hai thứ nữa là hành động và reward signal. Do đó, nói một cách toán học thì một chuỗi markov cần phải có 4 thứ sau:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\mathcal{S}$ là tập hợp các state&lt;/li&gt;
  &lt;li&gt;$\mathcal{A}$ là tập hợp các hành động có thể thực hiện&lt;/li&gt;
  &lt;li&gt;$P(s_t = j|s_t = i, a_t=a)$ là xác suất chuyển từ state $t$ đến state $t+1$ biết hành động $a$ được thực hiện tại thời điểm $t$ áp lên state $t$&lt;/li&gt;
  &lt;li&gt;$R(s, a) = \mathbb{E}[r_t | s_t=s, a_t = a]$ chính là kì vọng reward tại state $s_t$ biết hành động $a_t$ được thực hiện.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Trong trường hợp biết sẵn được mô hình tương tác giữa các state của môi trường với nhau, cách nhanh nhất và đơn giản nhất là vét cạn qua tất cả các state và action để tìm ra được policy tốt nhất. Song, như các bạn đã biết, mọi cách vét cạn đều có nhược điểm là độ phức tạp cực lớn do không gian tìm kiếm khủng. Nếu kí hiệu $|\mathcal{S}|$ là số state của môi trường và $|\mathcal{A}|$ là số hành động cụ thể. Như vậy độ phức tạp của thuật toán vét cạn phải tối thiểu $\mathcal{O}(|\mathcal{S}|^{|\mathcal{A}|})$. Đây là độ phức tạp mà siêu máy tính hiện tại vẫn chưa thể giải ra nếu số hành động khả thi lớn (số state có thể nhỏ trong hầu hết vấn đề). Vì vậy, hai thuật toán Policy Iteration và Value Iteration được ra đời để có thể tìm kiếm policy một cách hiệu quả hơn.&lt;/p&gt;

&lt;h3 id=&quot;41-thuật-toán-policy-iteration&quot;&gt;4.1. Thuật toán Policy Iteration&lt;/h3&gt;

&lt;p&gt;Để hiểu thuật toán Policy Iteration, trước hết chúng ta cần tổng quát hoá hàm value. Nếu các bạn học xác suất tốt thì có thể chứng minh được đẳng thức sau (mình sẽ chứng minh điều này trong phần Phụ lục):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\pi}(s) = R(s, \pi (s)) + \gamma\sum_{s'\in\mathcal{S}}P(s_{t+1}=s'|s_t=s,a_t=\pi (s))V^{\pi}(s')&lt;/script&gt;

&lt;p&gt;Đẳng thức này được gọi là đẳng thức Bellman và nó giúp cho việc tính toán hàm value function trở nên dễ dàng hơn thông qua việc sử dụng qui hoạch động. Nhìn vào đẳng thức trên, có thể thấy hàm $\pi (s)$ thực chất chính là hành động $a$ bất kì. Do đó, phương trình này có thể được tổng quát hoá thành một hàm số như sau:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^\pi (s, a) = R(s, a) + \gamma\sum_{s'\in\mathcal{S}} P(s_{t+1}=s'|s_t=s,a_t=a)V^{\pi}(s')&lt;/script&gt;

&lt;p&gt;Hàm số này còn có tên gọi khác là hàm state-action, đóng vai trò rất quan trọng trong việc tối ưu hoá các policy. Người ta chứng minh được rằng cho một policy $\pi$ bất kì và một policy $\pi’$ được tạo ra bằng cách: với mỗi state $s$, chọn $a$ sao cho $Q^\pi (s, a)$ đạt cực đại thì $V^{\pi’}(s)\geq V^\pi (s)$ (mình sẽ chứng minh trong phần phụ lục). Thuật toán Policy Iteration được xây dựng trên nguyên lí này bằng cách lâp đi lặp lại việc tạo ra $\pi’$ từ các policy cũ từ đó thu được một policy tối ưu. Mã giả của thuật toán được mô tả ở hình dưới đây:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/policy_eval.png&quot; alt=&quot;policy evaluation&quot; /&gt;
&lt;img src=&quot;./assets/pi_vi/policy_improvement.png&quot; alt=&quot;policy improvement&quot; /&gt;
&lt;img src=&quot;./assets/pi_vi/policy_iteration.png&quot; alt=&quot;policy iteration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nếu tính độ phức tạp của thuật toán này thì chúng ta giả sử ở thuật toán Policy Evaluation ta giới hạn lại số vòng lặp bằng 1 con số $N$ rất lớn và giả sử thuật toán Policy Iteration chạy với $k$ lần thì độ phức tạp của thuật toán sẽ là $\mathcal{O}(k\times N\times |\mathcal{S}|^2))$ nhỏ hơn so với việc vét cạn hết toàn bộ trường hợp rất nhiều. Tất nhiên cần phải lưu ý, tuy chặn trên của $k$ là $|\mathcal{S}|^{|\mathcal{A}|}$ nhưng trên thực tế, thuật toán Policy Iteration khám phá trong một không gian con nhỏ hơn rất nhiều so với việc vét cạn hết toàn bộ trường hợp. Tuy nhiên, một nhược điểm lớn nhất của thuật toán là tốc độ hội tụ tuy nhanh nhưng độ phức tạp của mỗi vòng lặp lại rất lớn (do thuật toán Policy Evaluation đòi hỏi một lượng lớn vòng lặp).&lt;/p&gt;

&lt;h3 id=&quot;42-thuật-toán-value-iteration&quot;&gt;4.2. Thuật toán Value Iteration&lt;/h3&gt;

&lt;p&gt;Thuật toán Value Iteration xuất phát từ một ý tưởng cực kì đơn giản nhưng lại rất hữu hiệu. Quan sát thuật toán Policy Iteration, ta có thể thấy thuật toán duy trì cả hai hàm policy và value cùng một lúc, khiến cho việc tính toán trở nên cồng kềnh và mất thời gian. Thuật toán Value Iteration bắt đầu bằng việc cố gắng tối ưu hàm value function trước rồi policy ứng với hàm value tối ưu đó đương nhiên cũng sẽ là policy tối ưu. Nếu quan sát kĩ hơn nữa trong thuật toán Policy Iteration thì có thể thấy thuật toán Policy Iteration xây dựng một chuỗi các policy $(V^{\pi_i})_{i\geq 0}$ tăng dần và hội tụ về $V^\pi$ tối ưu. Nhưng như bạn thấy, thực chất cũng chả cần phải có $\pi_i$ để có thể tối ưu hoá $V^\pi$ mà chỉ cần chọn ra tạo ra giá trị ban đầu $V(s)=0$ với mỗi $s$ và chọn ra hành động $a$ để cho hàm $Q(s, a)$ đạt giá trị lớn nhất và cứ lặp lại liên tục thì chắc chắn cũng sẽ thu được $V$ cực đại. Như vậy, ta có thuật toán Value Iteration đơn giản như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/value_iteration.png&quot; alt=&quot;value iteration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ở trong thuật toán này, mỗi bước của thuật toán có độ phức tạp là $\mathcal{O}(|\mathcal{A}|\times |\mathcal{S}|^2)$, thấp hơn so với mỗi bước của thuật toán Policy Iteration ($\mathcal{O}(N\times|\mathcal{S}|^2)$ với $N\gg|\mathcal{A}|$). Tuy nhiên, tốc độ hội tụ của thuật toán lại chậm hơn so với thuật toán trên.&lt;/p&gt;

&lt;h2 id=&quot;sử-dụng-thuật-toán-value-iteration-và-policy-iteration-vào-bài-toán-tìm-đường-đi-ngắn-nhất-trong-ma-trận-vuông&quot;&gt;Sử dụng thuật toán Value Iteration và Policy Iteration vào bài toán tìm đường đi ngắn nhất trong ma trận vuông&lt;/h2&gt;

&lt;p&gt;Trước hết mình cần định nghĩa bài toán ma trận vuông mà mình cần giải.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Bài toán:&lt;/strong&gt; Cho một ma trận vuông $n\times n$ với vị trí bắt đầu là ô vuông (0, 0) và vị trí kết thúc là ô vuông (n-1, n-1). Trong bảng ma trận, có một số ô vuông là cạm bẫy. Khi đi vào các ô cạm bẫy, xác suất để thoát ra được ô vuông đó là p (lưu ý là trong bài toán này agent dính bẫy sẽ không chết mà sẽ bị dính ở ô vuông đó cho đến khi thoát ra được, giống với một ô bùn). Hãy tìm đường đi hiệu quả để đi từ ô vuông ban đầu đến ô vuông kết thúc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dưới đây là một ví dụ minh hoạ cho bài toán:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/demo.png&quot; alt=&quot;minhoa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ở trên hình có 2 bảng vuông, bảng bên trái là bảng có chứa thông tin về các ô vuông bẫy (đỏ), ô bắt đầu (đen) và ô kết thúc (đen) và bảng bên phải sẽ là bảng thể hiện trạng thái của agent (đen). Mình sẽ kí hiệu $M(T, p)$ đại diện cho bài toán trên với bảng vuông $T$ với xác suất thoát bẫy là $p$. Vì mục đích bảo trì mã, mình xin phép mỗi bài test sẽ là một bảng vuông khác nhau để cho các bạn thấy thuật toán của mình hoạt động đúng. Hai thuật toán Policy Iteration và Value Iteration mình code không sử dụng quá nhiều thư viện cao siêu (chủ yếu là numpy) và link github phần code mình ở &lt;a href=&quot;https://github.com/minhrongcon2000/policy_iteration_and_value_iteration&quot;&gt;đây&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trước khi bước vào giải bài toán, mình sẽ phát biểu mô hình Markov của bài toán này:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\mathcal{S}$ sẽ bao gồm $n\times n$ toạ độ của các ô vuông với ô vuông bắt đầu là (0, 0) và ô vuông kết thúc $(n-1, n-1)$&lt;/li&gt;
  &lt;li&gt;$\mathcal{A}$ sẽ bao gồm 4 thao tác chính: lên, xuống, trái, phải.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Định nghĩa phân phối $P(s_{t+1}|s_t, a_t)$:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$P(s_{t+1} = (i’, j’) | s_t = (i, j), a_t=a) = 0$ nếu thực hiện hành động $a$ lên $(i, j)$ không thể đến được $(i’, j’)$&lt;/li&gt;
      &lt;li&gt;$P(s_{t+1} = (i’, j’) | s_t = (i, j), a_t=a) = p$ nếu $(i, j)$ là ô bẫy và $(i, j)$ đến được $(i’, j’)$ thông qua $a$ (tất nhiên điều đó có nghĩa là $P(s_{t+1}=(i, j) | s_t=(i, j), a_t=a) = 1-p$).&lt;/li&gt;
      &lt;li&gt;$P(s_{t+1} = (i’, j’) | s_t = (i, j), a_t=a) = 1$ nếu $(i, j)$ không là ô bẫy và $(i, j)$ đến được $(i’, j’)$ thông qua hành động $a$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reward: Trong phần test này mình sẽ test 2 hàm reward để cho các bạn thấy hàm reward sẽ ảnh hưởng nhiều đến nghiệm của bài toán. Hai hàm reward đó là:
    &lt;ul&gt;
      &lt;li&gt;$R_{l_1}((i, j), a) = \dfrac{1}{1 + |i - n + 1| + |j - n + 1|}$&lt;/li&gt;
      &lt;li&gt;$R_{l_2}((i, j), a) = \dfrac{1}{1 + \sqrt{(i - n + 1)^2 + (j - n + 1)^2}}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discounted factor $\gamma$: Mình sẽ test luôn cả ảnh hưởng của $\gamma$ lên các thuật toán để cho các bạn thấy thế nào là các policy ngắn hạn và dài hạn.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ảnh-hưởng-của-hàm-reward-lên-kết-quả-của-bài-toán&quot;&gt;Ảnh hưởng của hàm reward lên kết quả của bài toán&lt;/h3&gt;

&lt;p&gt;Trong phần này, mình sẽ không sử dụng các ô vuông bẫy để cho các bạn có thể thấy hai kết quả nó sẽ khác nhau như thế nào. Như vậy, bảng vuông $T$ của mình sẽ có dạng như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/reward_init.png&quot; alt=&quot;init_env&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nếu các bạn tinh ý và các bạn có kiến thức nền về toán, $R_{l_1}((i, j), a)$ thực chất là reward cho các ô vuông dựa trên độ xa gần của ô vuông đó đến ô vuông cuối theo khoảng cách $L_1$ và $R_{l_2}((i, j), a)$ thực chất là reward cho các ô vuông dựa trên độ xa gần của ô vuông đó đến ô vuông cuối theo khoảng cách $L_2$. Theo đó, nếu đúng theo lí thuyết, agent của chúng ta trong trường hợp $L_1$ sẽ đi men theo cạnh của bảng và trong trường hợp $L_2$ sẽ đi men theo đường chéo chính. Đây là kết quả của thuật toán Policy Iteration khi sử dụng hai reward khác nhau (với $\gamma = 0.9$).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;L1 reward&lt;/em&gt;
&lt;img src=&quot;./assets/pi_vi/l1_pi_case.png&quot; alt=&quot;l1_pi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;L2 reward&lt;/em&gt;
&lt;img src=&quot;./assets/pi_vi/l2_pi_case.png&quot; alt=&quot;l2_pi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thuật toán Value Iteration cũng cho ra kết quả tương tự (cũng với $\gamma$ như trên).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;L1 reward&lt;/em&gt;
&lt;img src=&quot;./assets/pi_vi/l1_vi_case.png&quot; alt=&quot;l1_pi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;L2 reward&lt;/em&gt;
&lt;img src=&quot;./assets/pi_vi/l2_vi_case.png&quot; alt=&quot;l2_pi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Điều này chứng tỏ rằng việc chọn hàm reward là một việc quan trọng để có thể tìm thấy nghiệm phù hợp với bài toán. Như các bạn thấy, việc chọn 2 công thức tính khoảng cách khác nhau đã tạo ra 2 nghiệm hoàn toàn khác nhau cho bài toán. Trong trường hợp bẫy xuất hiện, một số trường hợp sử dụng $L_1$ sẽ đảm bảo agent tìm được nghiệm bài toán hiệu quả hơn. Ví dụ như trong trường hợp dưới đây:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/l2_eff_case.png&quot; alt=&quot;l1_eff_case&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rõ ràng lúc này chiến thuật chỉ đi theo đường chéo chứng tỏ ưu thế của mình khi bẫy bố trí ở gần mép bản đồ. Tất nhiên, điều đó cũng có nghĩa nếu bẫy tập trung ở gần tâm bản đồ thì việc sử dụng $L_1$ lại trở nên hữu hiệu.&lt;/p&gt;

&lt;h3 id=&quot;ảnh-hưởng-của-discount-factor-lên-policy-tìm-ra&quot;&gt;Ảnh hưởng của discount factor lên policy tìm ra&lt;/h3&gt;

&lt;p&gt;Dưới đây là bảng $T$ mà mình sử dụng, mình sẽ test trên $L_2$ reward, $L_1$ xin để giành cho bạn đọc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/gamma_test.png&quot; alt=&quot;gamma_test&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Và dưới đây là kết quả của việc sử dụng Policy Iteration trên bảng $T$ với $\gamma=0.1$ và $\gamma=0.9$.&lt;/p&gt;

&lt;p&gt;$\gamma=0.1$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/low_gamma.png&quot; alt=&quot;low_gamma&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\gamma=0.9$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/pi_vi/high_gamma.png&quot; alt=&quot;high_gamma&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Như các bạn thấy, ở trường hợp trên, vì $\gamma$ nhỏ nên agent có xu hướng tối ưu hoá các bước đi ở những timestep gần nhất mà không để ý đến kết quả dài lâu, do đó agent vẫn đi theo lối cũ mặc dù sẽ dính rất nhiều cạm bẫy. Mặt khác, nếu bạn chọn $\gamma$ lớn như ở trường hợp dưới, agent sẽ suy nghĩ đến việc làm sao để các bước sau đi không gặp bẫy từ đó có thể đến được ô cuối.&lt;/p&gt;

&lt;h3 id=&quot;value-iteration-và-policy-iteration-hội-tụ-nhanh-hay-thực-thi-nhanh&quot;&gt;Value Iteration và Policy Iteration: Hội tụ nhanh hay thực thi nhanh?&lt;/h3&gt;

&lt;p&gt;Cuối cùng, mình muốn cho các bạn thấy về ưu khuyết điểm của thuật toán Policy Iteration và thuật toán Value Iteration. Mình cần nhắc là hai thuật toán này luôn đảm bảo độ hội tụ về nghiệm tối ưu, chứng minh sẽ ở phần Phụ lục. Mình test đơn giản trên con Mac OS X với processor 2.5 GHz Quad-Core Intel Core i7. Bảng $T$ lúc này sẽ là bảng không có bẫy như bài test đầu tiên nhưng với kích thước $10\times 10$. Lúc này, $\gamma=0.9$. Mình sẽ test cả 2 thuật toán trên cả $L_1$ và $L_2$ để cho thấy ưu nhược điểm của mỗi cái.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Test&lt;/th&gt;
      &lt;th&gt;#iterations&lt;/th&gt;
      &lt;th&gt;each iteration time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 + Policy Iteration&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;8.13s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L2 + Policy Iteration&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;8.51s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 + Value Iteration&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;0.13s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L2 + Value Iteration&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;0.13s&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Rõ ràng như các bạn đã thấy, thuật toán Value Iteration đòi hỏi nhiều vòng lặp (gấp 34.6 lần) thuật toán Policy Iteration để có thể hội tụ đến policy tối ưu nhưng bù lại, mỗi vòng lặp chỉ tiêu tốn khoảng 0.13s tức là nhỏ hơn gần 65 lần so với thuật toán Policy Iteration. Lí do như mình đã giải thích, thuật toán Policy Iteration tập trung vào việc tính toán hàm value rồi từ đó cải thiện policy, việc này đòi hỏi một lượng lặp rất lớn (trong trường hợp của mình là khoảng tầm 100 vòng lặp). Trong khi đó, thuật toán Value Iteration chỉ tập trung tối ưu hoá hàm value rồi trích xuất policy từ đó, do đó, lượng công thức tính toán ít nhưng đổi lại tốc độ hội tụ khá chậm. Thuật toán value iteration sẽ rất thích hợp trong các bài toán finite-horizon, tức là các bài toán đòi hỏi giới hạn về số bước thực hiện vì lúc này environment sẽ bị giới hạn, do đó sử dụng một lượng lớn vòng lặp chỉ để tính policy tối ưu không phải là một phương án hay. Và ngược lại, Policy Iteration lại phù hợp cho các bài toán không giới hạn về bước thực hiện mà chỉ quan tâm đến kết quả vì lúc này chi phí lấy mẫu từ environment đã trở nên rất đắt.&lt;/p&gt;

&lt;h2 id=&quot;5-kết-luận&quot;&gt;5. Kết luận&lt;/h2&gt;

&lt;p&gt;Vậy là mình đã hoàn thành việc giới thiệu sơ qua cho các bạn về bài toán Reinforcement Learning và cách giải bài toán này trong trường hợp biết mô hình tương tác giữa các state trong môi trường (hay còn gọi là mô hình chuỗi Markov). Đó chính là hai thuật toán Value Iteration và Policy Iteration. Cả hai thuật toán đều tốt hơn so với cách vét cạn nhưng lại chú trọng vào hai đối tượng khác nhau. Policy Iteration chú trọng vào tối ưu hoá một policy chưa tối ưu thông qua việc cực đại hoá hàm state action. Ngược lại, Value Iteration chú trọng vào việc cực đại hoá hàm state value và thông qua đó trích xuất policy tối ưu. Mình đã test cho các bạn thấy ưu nhược điểm của cả hai: Policy Iteration hội tụ nhanh những chi phí mỗi vòng lặp đắt trong khi Value Iteration lại hội tụ lâu với chi phí mỗi vòng lặp rẻ. Tuy nhiên, cả hai thuật toán này chỉ hữu dụng khi giải quyết các bài toán đã biết trước mô hình. Trong trường hợp chưa biết trước mô hình, chúng ta cần các kĩ thuật khác mà mình sẽ đề cập ở bài viết sau.&lt;/p&gt;

&lt;h2 id=&quot;phụ-lục&quot;&gt;Phụ lục&lt;/h2&gt;

&lt;p&gt;Như bài blog trước của mình, phần này sẽ dành cho những bạn thích Toán và muốn biết thêm tại sao hai thuật toán trên có thể hoạt động đúng và đảm bảo hội tụ. Trước hết, chúng ta cần chứng minh rằng tồn tại policy tối ưu để hàm $V^{\pi_i}(s)$ đạt giá trị lớn nhất với mọi $s$. Điều này có thể dễ dàng suy ra được vì chỉ có hữu hạn hành động và state nên sẽ tồn tại một tập các hành động ứng với state để hàm value lớn nhất.&lt;/p&gt;

&lt;p&gt;Tiếp theo, như mình nói ở phần 4.1, thuật toán Policy Iteration hoạt động dựa trên nguyên tắc liên tục cải thiện policy chưa tối ưu. Do đó mình cần phải chứng minh bổ đề sau:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bổ đề 1:&lt;/strong&gt; $(V^{\pi_i}(s))$ là một dãy tăng với mọi $s\in\mathcal{S}$.&lt;/p&gt;

&lt;p&gt;Chứng minh: Mình kí hiệu $G_t=r_t+\gamma r_{t+1}+ \gamma^2r_{t+2} + \ldots$. Như vậy, dễ thấy $V^{\pi_i}(s) = \mathbb{E}[G_t|s_t=s, a_t=\pi_i(s)] = Q^{\pi_i}(s, \pi(s))$. Theo thuật toán Policy Iteration thì&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
    V^{\pi_i}(s) &amp;= Q^{\pi_i}(s, \pi(s))\\
    &amp;\leq\max_a Q^{\pi_i}(s, a)\\
    &amp;=Q^{\pi_i}(s, \pi_{i+1}(s))\\
    &amp;=\mathbb{E}[G_t|s_t=s, a_t=\pi_{i+1}(s)]\\
    &amp;=\mathbb{E}[r_t + \gamma G_{t+1}|s_t=s, a_t=\pi_{i+1}(s)]\\
    &amp;=\mathbb{E}[r_t + \gamma V^{\pi_i}(s_{t+1})|s_t=s, a_t=\pi_{i+1}(s)]\\
    &amp;\leq\mathbb{E}[r_t + \gamma\max_a Q^{\pi_i}(s_{t+1}, a)|s_t=s, a_t=\pi_{i+1}(s)]\\
    &amp;=\mathbb{E}[r_t + \gamma r_{t+1} + \gamma^2 V^{\pi_i}(s_{t+2}) | s_t=s, a=\pi_{i+1}(s)]\\
    &amp;\ldots\\
    &amp;\leq\mathbb{E}[r_t + \gamma r_{t+1} + \ldots | s_t=s, a_t=\pi_{i+1}(s)]\\
    &amp;=V^{\pi_{i+1}}(s)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Từ đó, ta có điều phải chứng minh (lưu ý, biến đổi từ dòng 3 sang 4 và 6 sang 7 là hệ quả của kì vọng lặp).&lt;/p&gt;

&lt;p&gt;Chính vì $(V^{\pi_i}(s))_{i\geq 0}$ là một dãy tăng và bị chặn trên nên nó hội tụ về chặn trên đúng của dãy đó là policy tối ưu (hệ quả của định lí Bolzano-Weierstrass). Từ đó, ta chứng minh được thuật toán Policy Iteration hội tụ.&lt;/p&gt;

&lt;p&gt;Để chứng minh thuật toán Value Iteration luôn hội tụ thì ta cần một số bổ đề sau:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bổ đề 2:&lt;/strong&gt; Chứng minh rằng với mọi hàm số $f$ và $g$ ta có&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|max f - max g|\leq max|f - g|&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Chứng minh&lt;/em&gt;: Gọi $x^*=\arg\max_x f$ và không mất tính tổng quát, giả sử $max f\geq\max g$. Từ đó, ta có $VT = f(x^*) - \max g\leq f(x^*) - g(x^*)\leq\max |f - g|.$&lt;/p&gt;

&lt;p&gt;Như vậy, theo thuật toán Value Iteration, ta sẽ có được:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
    \lVert V_{i+1}(s) - V_i(s)\rVert_\infty &amp;= \max_{s\in\mathcal{S}} |V_{i+1}(s) - V_i(s)|\\
    &amp;= \max_{s\in\mathcal{S}} |\max_{a\in\mathcal{A}} R(s, a) + \gamma \sum_{s'\in\mathcal{S}} P(s'|s, a)V_i(s') - \max_{a\in\mathcal{A}} R(s, a) + \gamma \sum_{s'\in\mathcal{S}} P(s'|s, a)V_{i-1}(s')|\\
    &amp;\leq\max_{s\in\mathcal{S}}\max_{a\in\mathcal{A}}\gamma\left|\sum_{s'\in\mathcal{S}}P(s'|s, a)(V_i(s') - V_{i-1}(s'))\right|\\
    &amp;\leq\max_{s\in\mathcal{S}}\max_{a\in\mathcal{A}}\gamma\left|\sum_{s'\in\mathcal{S}}P(s'|s, a)\lVert V_i(s') - V_{i-1}(s')\rVert_\infty\right|\\
    &amp;=\max_{s\in\mathcal{S}}\max_{a\in\mathcal{A}}\gamma\lVert V_i(s') - V_{i-1}(s')\rVert_\infty\left|\sum_{s'\in\mathcal{S}}P(s'|s, a)\right|\\
    &amp;=\gamma\lVert V_i(s') - V_{i-1}(s')\rVert_\infty\\
    &amp;\leq\gamma^2\lVert V_{i-1}(s') - V_{i-2}(s')\rVert_\infty\\
    &amp;\ldots\\
    &amp;\leq\gamma^i\lVert V_1(s') - V_0(s')\rVert_\infty
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Cho $i\to\infty$ thì ta có được $\lim_{i\to\infty}\lVert V_{i+1}(s) - V_i(s)\rVert_\infty = 0$. Từ đó suy ra được thuật toán này hội tụ về hàm value tối ưu và từ đó có thể trích xuất ra policy tối ưu.&lt;/p&gt;</content><author><name>Pham Hoang Minh</name></author><category term="maths" /><category term="optimization" /><category term="reinforcement-learning" /><category term="policy-iteration" /><category term="value-iteration" /><summary type="html">1. Giới thiệu</summary></entry><entry><title type="html">Toán học đằng sau thuật toán hồi qui tuyến tính (Linear Regression).</title><link href="http://localhost:4000/linear-regression" rel="alternate" type="text/html" title="Toán học đằng sau thuật toán hồi qui tuyến tính (Linear Regression)." /><published>2021-06-26T00:00:00+07:00</published><updated>2021-06-26T00:00:00+07:00</updated><id>http://localhost:4000/linear-regression</id><content type="html" xml:base="http://localhost:4000/linear-regression">&lt;h2 id=&quot;giới-thiệu&quot;&gt;Giới thiệu&lt;/h2&gt;

&lt;p&gt;Trong thời gian học Đại học của mình, chương trình học có một môn học tên là Regression Analysis. Tên rất kêu nhưng giáo viên dạy môn học này của mình rất qua loa và không chú ý đến chi tiết của các định lí. Vậy nên, lần này, mình sẽ viết góc nhìn xác suất của các thuật toán hồi qui tuyến tính nhằm chỉ ra rằng việc sử dụng các thuật toán tối ưu hoá là chưa đủ để có thể tạo ra các mô hình học máy hoàn chỉnh. Bài viết này mình chia thành 3 phần: Phần 1 là động cơ để mình viết bài blog này, phần 2 mình sẽ giới thiệu sơ về thuật toán hồi qui tuyến tính, phần 3 sẽ là về góc nhìn xác suất của thuật toán và làm sao để sử dụng thuật toán này với hiệu quả cao nhất và phần cuối cùng sẽ là kết luận bao gồm một số nhược điểm của thuật toán này.&lt;/p&gt;

&lt;h2 id=&quot;thuật-toán-hồi-qui-tuyến-tính-linear-regression&quot;&gt;Thuật toán hồi qui tuyến tính (Linear Regression)&lt;/h2&gt;

&lt;p&gt;Có thể nói, thuật toán hồi qui tuyến tính là một trong những thuật toán cơ bản nhất trong tất cả các thuật toán học máy sử dụng tham số có giám sát (là các thuật toán xây dựng mô hình học máy bằng các giả định về mối quan hệ giữa nhãn (label) và các thuộc tính (attributes)). Ngay cả các thuật toán mạng nơ-ron (neural network) cũng được xây dựng từ thuật toán này mà ra. Vậy nên, hiểu được cách vận hành của thuật toán này sẽ giúp bạn cảm thấy dễ hiểu các thuật toán máy học cao hơn.&lt;/p&gt;

&lt;p&gt;Thuật toán hồi qui tuyến tính như tên gọi xây dựng dựa trên giả định rằng nhãn có mối quan hệ tuyến tính với các thuộc tính. Phát biểu một cách toán học thì nếu gọi $y$ là biến cần dự đoán (ví dụ như giá nhà chẳng hạn) và $x_1$, $x_2$, $\ldots$, $x_n$ là các thuộc tính mà có thể biến $y$ sẽ phụ thuộc vào (như giá nhà thì phụ thuộc vào số phòng của ngôi nhà, nhà cao bao nhiêu tầng, diện tích nhà rộng không, nhà có bao nhiêu mặt tiền, vân vân) thì thuật toán hồi qui tuyến tính giả sử rằng tồn tại các tham số $w_0$, $w_1$, $\ldots$, $w_n$ sao cho&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
    \widehat{y} = w_0 + w_1x_1 + w_2x_2 + \ldots + w_nx_n.
\end{align*}&lt;/script&gt;

&lt;p&gt;Trong giới khoa học, một bộ $(w_0, w_1, \ldots, w_n)$ được gọi là một bộ trọng số. Mục đích của thuật toán hồi qui tuyến tính là để tìm ra bộ trọng số tối ưu để ước lượng mối quan hệ tuyến tính giữa nhãn và thuộc tính dựa trên một tập dữ liệu có sẵn. Nhưng khi đã dùng đến từ tối ưu thì phải có một “tiêu chí” để đánh giá xem bộ trọng số nào tối ưu hơn bộ trọng số nào. Và “tiêu chí” đó chính là hàm mean-squared error để tính sai số giữa dự đoán của mô hình ($\widehat{y}$) và nhãn ($y$).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Giả sử ta có một tập dữ liệu $\mathcal{D} = \{(x_{i, 1}, x_{i, 2}, \ldots, x_{i, n}, y_i)|i=\overline{1,m}\}$ với $x_{i, j}$ là giá trị ở thuộc tính $j$ của ví dụ $i$ và $y_i$ chính là nhãn tương ứng. Với mỗi bộ trọng số $(w_1, w_2, \ldots, w_n)\in\mathbb{R}^n$ thì hàm mean-squared error ứng với bộ tham số đó được định nghĩa:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(w_1, w_2, \ldots, w_n) = \dfrac{1}{m}\sum_{i=1}^m\left(y_i - \widehat{y}_i\right)^2&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;với $\widehat{y}_i = w_0 + w_{1,i}+ w_2x_{2, i} + \ldots + w_nx_{n, i} $.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Có thể nhìn thấy mean-squared error có hơi hướng giống với khoảng cách giữa 2 điểm trên hệ toạ độ Decartes n chiều. Như vậy, rõ ràng bộ trọng số càng tối ưu thì mean-squared error ứng với nó càng nhỏ và ngược lại. Bằng việc sử dụng giải tích, ta có thể chứng minh rằng $J$ là một hàm lồi liên tục trên $R^n$, do đó, chúng ta có thể áp dụng thuật toán Gradient Descent để tìm ra nghiệm của bài toán (các bạn có thể xem bài viết về Gradient Descent của mình tại &lt;a href=&quot;./gradient-descent&quot;&gt;đây&lt;/a&gt;). Nếu các bạn có kiến thức về giải tích đa chiều, có thể tìm được vector Jacobi của $J$ là&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\overrightarrow{\nabla} J(\overrightarrow{w})=X^T(X\overrightarrow{w} - \overrightarrow{y})&lt;/script&gt;

&lt;p&gt;với&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X = \left[
        \begin{array}{l}
            1 &amp; x_{1, 1} &amp; x_{1, 2} &amp; \cdots &amp; x_{1, n}\\
            1 &amp; x_{2, 1} &amp; x_{2, 2} &amp; \cdots &amp; x_{2, n}\\
            \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            1 &amp; x_{m, 1} &amp; x_{m, 2} &amp; \cdots &amp; x_{m, n}
        \end{array}
    \right], \overrightarrow{w} = \left[
        \begin{array}{l}
            w_1\\
            w_2\\
            \vdots\\
            w_n
        \end{array}
    \right], \text { và } \overrightarrow{y} = \left[
        \begin{array}{l}
            y_1\\
            y_2\\
            \vdots\\
            y_m
        \end{array}
    \right]\tag{1}\label{eq:01} %]]&gt;&lt;/script&gt;

&lt;p&gt;Như vậy, thuật toán Gradient Descent cho bài toán hồi qui tuyến tính có thể được thiết lập như sau:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Khởi tạo giá trị ban đầu $w_0$ và thiết lập ma trận $X$ và vector $y$ như trên.&lt;/li&gt;
  &lt;li&gt;Thực hiện một lượng lớn vòng lặp cho đến khi $\overrightarrow{w}_n$ hội tụ. Tại vòng lặp thứ i, thực hiện các bước sau:
    &lt;ol&gt;
      &lt;li&gt;Tính vector Jacobi ứng với $\overrightarrow{w}_{i-1}$: $\overrightarrow{\nabla} J(\overrightarrow{w}_{i-1}) = X^T\left(X\overrightarrow{w}_{i-1} - \overrightarrow{y}\right).$&lt;/li&gt;
      &lt;li&gt;Tính trọng số mới tối ưu hơn trọng số cũ $\overrightarrow{w}_i = \overrightarrow{w}_{i-1} - \alpha * \overrightarrow{\nabla} J(\overrightarrow{w}_{i-1})$ với $\alpha$ là tốc độ học (learning rate).&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Như vậy sau một lượng lớn vòng lặp thì vector $\overrightarrow{w}_n$ sẽ hội tụ đến nghiệm tối ưu.&lt;/p&gt;

&lt;p&gt;Để lấy ví dụ ứng dụng thực tế của thuật toán, giả sử bạn cần ước lượng giá ngôi nhà của bạn với diện tích khoảng 100 mét vuông trên thị trường để bán căn nhà của bạn. Bước đầu tiên của bạn luôn luôn phải là khảo sát thị trường thu thập giá của nhiều ngôi nhà với diện tích khác nhau. Giả sử bước này đã thực hiện xong và bạn có tập dữ liệu như mô tả bên dưới.&lt;/p&gt;

&lt;center&gt;
    &lt;table&gt;
        &lt;thead&gt;
            &lt;tr&gt;
                &lt;th&gt;Diện tích (m2)&lt;/th&gt;
                &lt;th&gt;Giá (triệu VNĐ)&lt;/th&gt;
            &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
            &lt;tr&gt;
                &lt;td&gt;1&lt;/td&gt;
                &lt;td&gt;2.72&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;6&lt;/td&gt;
                &lt;td&gt;4.47&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;12&lt;/td&gt;
                &lt;td&gt;7.6&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;17&lt;/td&gt;
                &lt;td&gt;10.76&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;23&lt;/td&gt;
                &lt;td&gt;12.92&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;28&lt;/td&gt;
                &lt;td&gt;19.02&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;34&lt;/td&gt;
                &lt;td&gt;18.17&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;40&lt;/td&gt;
                &lt;td&gt;22.5&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;45&lt;/td&gt;
                &lt;td&gt;21.06&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;50&lt;/td&gt;
                &lt;td&gt;23.84&lt;/td&gt;
            &lt;/tr&gt;
        &lt;/tbody&gt;
    &lt;/table&gt;
&lt;/center&gt;

&lt;p&gt;Khi lấy trục x là diện tích nhà và trục y là giá nhà thì bạn sẽ có hình dưới đây.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;./assets/linear_reg.png&quot; width=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Rõ ràng biểu đồ trên cho ta thấy mỗi điểm dữ liệu gần như nằm trên một đường thẳng. Và chúng ta cần phải tìm đường thẳng đó để có thể ước lượng giá nhà của mình. Như vậy đầu tiên bạn phải thiết lập ma trận $X$ và vector $\overrightarrow{y}$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X = \left[
        \begin{array}{l}
            1 &amp; 1 \\
            1 &amp; 6 \\
            1 &amp; 12 \\
            1 &amp; 17 \\
            1 &amp; 23 \\
            1 &amp; 28 \\
            1 &amp; 34 \\
            1 &amp; 40 \\
            1 &amp; 45 \\
            1 &amp; 50 \\
        \end{array}
    \right]\text{ và }
    \overrightarrow{y}=\left[
        \begin{array}{l}
            2.72\\
            4.47\\
            7.6\\
            10.76\\
            12.92\\
            19.02\\
            18.17\\
            22.5\\
            21.06\\
            23.84
        \end{array}
    \right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Sau khi đã thiết lập $X$ và $\overrightarrow{y}$, bạn chỉ cần khởi tạo $w_0$ có chiều $2\times 1$ và tạo một vòng lặp (tầm 500 vòng) thực hiện các bước như đã đề cập ở trên là sẽ tìm được bộ tham số tối ưu để ước lượng giá nhà của bạn. Dưới đây là một ví dụ của việc sử dụng Python để chạy bài toán trên với 500 vòng lặp.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# mình sử dụng thư viện numpy để hỗ trợ việc tính ma trận
# các bạn có thể tham khảo nó tại: https://numpy.org
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Đầu tiên, mình thiết lập ma trận X và vector y
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;34&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.72&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;7.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.76&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;12.92&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;19.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;18.17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;22.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;21.06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;23.84&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Sau đó mình khởi tạo vector w. 
# Ở đây mình cứ chọn đại vector 0 có chiều 2x1
# Các bạn có thể khởi tạo random
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Cuối cùng mình chạy 1 vòng lặp 500 vòng 
# và thu được nghiệm của bài toán
# Các bạn có thể sử dụng các điều kiện dừng khác như
# nếu bộ tham số ở iteration sau không lớn hơn nhiều so với bộ tham số trước thì dừng
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jacob_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jacob_vector&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Nếu bạn muốn predict giá nhà thì chạy đoạn code này là có thể tìm được giá nhà của bạn.
# Thế là bạn có thể bán nhà thành công :D
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Dưới đây là hình minh hoạ cho quá trình huấn luyện của thuật toán hồi qui tuyến tính.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;./assets/linear_reg_result.png&quot; width=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Như các bạn thấy, nếu thuật toán của các bạn được thực thi thành công thì khi bạn vẽ đường thẳng dự đoán, khoảng cách giữa mỗi điểm đến đường dự đoán rất nhỏ (hình trên). Hơn nữa, để có thể phát hiện ra sai sót trong quá trình thực thi, các bạn có thể vẽ biểu đồ đường với trục x thể hiện vòng lặp và trục y thể hiện giá trị của hàm mất mát. Nếu tốc độ học được chọn phù hợp và bạn không tính sai đạo hàm, biểu đồ sẽ có dạng giảm dần vào những vòng lặp ban đầu và phẳng dần với những vòng lặp lớn hơn (hình dưới).&lt;/p&gt;

&lt;p&gt;Nếu bạn đọc được đến đây rồi thì chúc mừng! Bạn đã có đủ kiến thức để bước tiếp vào các thuật toán cao hơn rồi. Đó là những gì bạn được nghe khi bạn học được đến đây từ những người dạy ở ngoài. Tuy nhiên, với mình, điều này là chưa đủ để tận dụng hết sức mạnh của thuật toán hồi qui tuyến tính. Làm sao để có thể hiểu hết và tận dụng hết được thuật toán này thì mời các bạn đến phần tiếp theo.&lt;/p&gt;

&lt;h2 id=&quot;góc-nhìn-xác-suất-của-thuật-toán-hồi-qui-tuyến-tính&quot;&gt;Góc nhìn xác suất của thuật toán hồi qui tuyến tính&lt;/h2&gt;

&lt;p&gt;Có thể các bạn không biết (hoặc đã biết) nhưng hầu hết các thuật toán học giám sát có sử dụng tham số đều được xây dựng dựa trên phương pháp maximum likelihood estimation (MLE, tên tiếng Việt là hợp lí cực đại nhưng mà tên này nó sai quá nên thôi mình giữ phiên bản tiếng Anh của nó). Phương pháp này cụ thể đưa ra một ước lượng tốt nhất về một tham số chưa biết bằng việc cực đại hoá phân phối của dữ liệu thu được nếu biết dữ liệu được lấy mẫu từ một phân bố xác suất bị chi phối bởi tham số chưa biết đó (likelihood). Ví dụ, bạn muốn tính chiều cao trung bình của người Việt Nam thì bạn phải lấy trung bình của 96 triệu dân. Thế nhưng bạn không thể đủ sức làm được việc đó (ngay cả việc khảo sát dân số cũng không thể thực hiện trên toàn bộ dân số Việt Nam mà chỉ thực hiện trên một lượng lớn dân số). Như vậy, việc bạn làm là đi hỏi khoảng 50-100 người Việt Nam về chiều cao và tính trung bình rồi cho ra kết quả. Việc tính trung bình đó là kết quả của việc sử dụng phương pháp MLE trên phân phối của dữ liệu bạn thu được biết dữ liệu được lấy mẫu từ một phân phối chuẩn với giá trị trung bình là tham số cần tìm và phương sai được xem là đã biết dựa trên các nghiên cứu thực nghiệm.&lt;/p&gt;

&lt;p&gt;Trong khuôn khổ bài viết này, mình sẽ sử dụng phương pháp này để cho các bạn thấy tại sao phải sử dụng hàm mean-squared error như trên để tìm ra bộ tham số tối ưu và những suy diễn liên quan đến nó (bạn cũng có thể áp dụng phương pháp này để tìm chiều cao trung bình của người Việt Nam). Để sử dụng phương pháp này trước hết chúng ta cần phát biểu lại bài toán hồi qui tuyến tính một chút để các bạn có thể thấy khía cạnh xác suất của nó:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Giả sử ta có $n$ biến ngẫu nhiên tương ứng với $n$ thuộc tính trong dữ liệu $X_1$, $X_2$, $\ldots$, $X_n$ và một biến Y có quan hệ tuyến tính với các thuộc tính hay nói cách khác (đây là giả sử của bài toán hồi qui tuyến tính):&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y=\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n+\epsilon&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sau đó, chúng ta lấy mẫu ngẫu nhiên dựa trên phân phối của $X_1, X_2, X_3, \ldots, X_n, Y$ và thu được một tập dữ liệu $\mathcal{D} = \{(x_{i,1}, x_{i,2},\ldots, x_{i,n}, y_i)| i=\overline{1,m}\}$ (đây chính là bước thu thập dữ liệu).&lt;/p&gt;

  &lt;p&gt;Như vậy, chúng ta cần phải tìm bộ tham số $(\widehat{\beta}_1, \widehat{\beta}_2, \ldots, \widehat{\beta}_n)$ các bộ tham số này &lt;em&gt;“gần”&lt;/em&gt; với bộ tham số $(\beta_0, \beta_1,\ldots,\beta_n)$ nhất dựa trên tập dữ liệu $\mathcal{D}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Để có thể sử dụng phương pháp MLE, chúng ta cần phải tìm phân phối tham số này biết dữ liệu mà chúng ta thu được $\mathcal{D}$. Nói một cách toán học tức là bạn phải tính được hàm phân phối xác suất $p(d_1, d_2,\ldots, d_n | \beta_1, \beta_2, \ldots, \beta_n)$ với $d_i = (x_{i,1}, x_{i,2},\ldots, x_{i,n}, y_i)$. Ở đây, nếu chúng ta giả sử $\epsilon\sim\mathcal{N}(0, \sigma^2)$ thì rõ ràng&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y\sim p(y)=\mathcal{N}(\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n, \sigma^2).&lt;/script&gt;

&lt;p&gt;Như vậy nếu $y_1$, $y_2$, $\ldots$, $y_m$ được lấy mẫu ngẫu nhiên độc lập từ $p(y)$ thì rõ ràng&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
    p(d_1, d_2, \ldots, d_n|\beta_0, \beta_1, \ldots, \beta_n) &amp;= \prod_{i=1}^m p(d_i|\beta_0, \beta_1, \ldots, \beta_n)\\
    &amp;= \prod_{i=1}^m \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\dfrac{(y_i - \beta_0 - \beta_1x_{i,1} - \beta_2x_{i,2} - \ldots - \beta_nx_{i,n})^2}{2\sigma^2}\right\}\\
    &amp;\propto\exp\left\{-\dfrac{\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_{i,1} - \beta_2x_{i,2} - \ldots - \beta_nx_{i,n})^2}{2\sigma^2}\right\}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Hàm $p(d_1, d_2, \ldots, d_n|\beta_0, \beta_1, \ldots, \beta_n)$ còn có một tên gọi khác nữa là hàm likelihood. Như vậy, như tên gọi của phương pháp MLE, chúng ta cần phải tìm bộ tham số $(\widehat{\beta}_1, \widehat{\beta}_2, \ldots, \widehat{\beta}_n)$ để hàm likelihood đạt giá trị lớn nhất. Chúng ta có thể tính đạo hàm trực tiếp của hàm likelihood để tìm ra kết quả hoặc lấy log hai vế rồi tìm giá trị cực đại của hàm log (lưu ý lấy log của một hàm số không làm thay đổi tính biến thiên, lồi lõm của hàm số đó).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
    \log p(d_1, d_2, \ldots, d_n|\beta_0, \beta_1, \ldots, \beta_n) = -\dfrac{1}{2\sigma^2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_{i,1} - \beta_2x_{i,2} - \ldots - \beta_nx_{i,n})^2
\end{align*}&lt;/script&gt;

&lt;p&gt;Đây rõ ràng tương ứng với việc cực tiểu hoá mean-squared error sử dụng thuật toán Gradient Descent như đã đề cập ở phía trước và cũng là nguồn gốc của việc sử dụng mean-squared error để tìm ra bộ tham số tốt nhất. Hơn nữa, chúng ta còn có thể biết nghiệm tối ưu chuẩn tắc của bài toán trên là $\widehat{\beta} = (X^TX)^{-1}X^Ty$. Tuy nhiên việc sử dụng nghiệm chuẩn tắc để giải bài toán hồi qui tuyến tính không được khuyến khích để sử dụng trong ứng dụng thực tế vì độ phức tạp của việc tính ma trận nghịch đảo không phù hợp để triển khai.&lt;/p&gt;

&lt;p&gt;Chính vì bản chất của hàm mean-squared error là việc sử dụng phương pháp MLE. Cho nên chúng ta cần phải biết được:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Ước lượng về tham số $\beta$ này có unbiased không (nói một cách khác là kì vọng của ước lượng $\widehat{\beta}$ phải đúng bằng với tham số $\beta$ chưa biết)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Làm sao để tính được khoảng tin cậy (confidence interval) của ước lượng này để có thể đánh giá liệu mô hình này có thể ứng dụng cho thực tế hay không?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Để trả lời câu hỏi 1, tất nhiên chúng ta phải tính kì vọng của $\widehat{\beta}$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[\widehat{\beta}] = \mathbb{E}[(X^TX)^{-1}X^Ty] = (X^TX)^{-1}X^T\mathbb{E}[y] = (X^TX)^{-1}X^TX\beta = \beta\tag{2}\label{eq:02}&lt;/script&gt;

&lt;p&gt;Như vậy có thể thấy được $\widehat{\beta}$ là một ước lượng unbiased. Câu hỏi thứ hai được trả lời từ hai quan sát sau (mình sẽ chứng minh nó trong phần &lt;a href=&quot;#phụ-lục&quot;&gt;Phụ lục&lt;/a&gt;).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;span id=&quot;first-obs&quot;&gt;$\widehat{\beta}\sim\mathcal{N}\left(\beta, \sigma^2(X^TX)^{-1}\right)$&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span id=&quot;second-obs&quot;&gt;Đặt $S = \lVert y - X\widehat{\beta}\rVert_2^2$. Như vậy thì $\dfrac{S}{\sigma^2}\sim\chi_{m-n-1}^2$.&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Như vậy, rõ ràng nếu đặt A = $(X^TX)^{-1}$ thì $\widehat{\beta}_i\sim\mathcal{N}(\beta_i, \sigma^2\times A_{ii})$, do đó&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\widehat{\beta_i} - \beta_i}{\sqrt{\dfrac{A_{ii}\times S}{m-n-1}}}\sim T_{m-n-1}&lt;/script&gt;

&lt;p&gt;Như vậy, với mức độ tin cậy $1-\alpha$, khoảng tin cậy của $\widehat{\beta_i}$ sẽ là&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[\widehat{\beta}_i-t_{\alpha/2, m-n-1}\sqrt{\dfrac{A_{ii}\times S}{m-n-1}}, \widehat{\beta}_i+t_{\alpha/2, m-n-1}\sqrt{\dfrac{A_{ii}\times S}{m-n-1}}\right]&lt;/script&gt;

&lt;p&gt;Ngoài ra, nếu bạn áp dụng khoảng tin cậy lên $\widehat{y} = X\widehat{\beta}$, bạn cũng có thể tìm ra được khoảng tin cậy của nó và thu được hình bên dưới về kết quả của thuật toán với ví dụ bên trên với khoảng tin cậy $95\%$.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;./assets/prediction_ci.png&quot; width=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Đây chính là góc nhìn đầy đủ và toàn diện nhất về bài toán hồi qui tuyến tính. Việc sử dụng phân phối mẫu của ước lượng tham số sẽ giúp bạn quyết định xem mô hình tuyến tính bạn xây dựng được có đáng tin cậy hay không. Nếu mô hình ước lượng được các tham số $\widehat{\beta}_i$ nhưng lại có khoảng tin cậy rất lớn thì có khả năng mô hình của bạn đã không mô hình hoá tốt mối quan hệ giữa nhãn và thuộc tính, ngược lại, nếu khoảng tin cậy nhỏ thì mô hình xây dựng được là đáng tin cậy và có thể sử dụng vào các ứng dụng thực tế.&lt;/p&gt;

&lt;p&gt;Hiểu biết được khía cạnh xác suất của bài toán hồi qui tuyến tính còn cho phép bạn lọc bỏ những thuộc tính không cần thiết. Điều này được thực hiện bằng việc sử dụng kiểm định giả thuyết xác suất (hypothesis testing) với giả thuyết rỗng (null hypothesis) $H_0: \beta_i=0$ và giả thuyết thay thế (alternative hypothesis) $H_a: \beta_i\neq 0$. Như vậy, với hệ số sig (significant level) $\alpha$,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Nếu $|\widehat{\beta}_i| &amp;gt; t_{\alpha / 2, m-n-1}\sqrt{\dfrac{A_{ii}\times S}{m - n - 1}}$ thì ta bác bỏ giả thuyết rỗng và chấp nhận giả thuyết thay thế. Tức là có một mối liên hệ giữa nhãn và thuộc tính $i$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ngược lại, nếu $|\widehat{\beta}_i| \leq t_{\alpha / 2, m-n-1}\sqrt{\dfrac{A_{ii}\times S}{m - n - 1}}$ thì ta chấp nhận giả thuyết rỗng. Tức là không hề có mối liên hệ nào giữa nhãn và thuộc tính i. Và ta có thể bỏ thuộc tính đó đi và xây dựng lại mô hình.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h2&gt;

&lt;p&gt;Việc sử dụng xác suất thống kê lên thuật toán hồi qui tuyến tính giúp cho thuật toán này trở nên cực kì hữu dụng không chỉ trong việc tìm ra mối tương quan giữa các biến với nhau mà còn giúp loại bỏ những biến không quan trọng gây nhiễu cho quá trình huấn luyện. Thuật toán hồi qui tuyến tính đã được sử dụng để làm nền cho rất nhiều thuật toán cao cấp sau này, bao gồm cả các mạng nơ-ron phức tạp. Tuy nhiên, thuật toán hồi qui tuyến tính vẫn còn nhiều hạn chế. Hạn chế đầu tiên là thuật toán hồi qui tuyến tính, đúng như tên gọi của nó, chỉ có thể biểu diễn được các mối quan hệ tuyến tính mà thiếu khả năng biểu diễn các mối quan hệ phi tuyến. Việc này tuy có thể cải thiện bằng việc phức tạp hoá thuộc tính bằng cách thêm các thuộc tính bậc cao nhưng lại dẫn đến tiềm tàng bị overfit trong quá trình huấn luyện. Hạn chế thứ hai là thuật toán hồi qui tuyến tính cực kì nhạy cảm với các điểm dữ liệu ngoại vi (outlier). Lí do là các hạng tử ứng với các điểm dữ liệu ngoại vi trong mean-squared error thường mang các giá trị rất lớn (do bình phương của một số cực lớn sẽ tạo ra một số lớn hơn), do đó việc xử lí dữ liệu là tối quan trọng trước khi áp dụng thuật toán này.&lt;/p&gt;

&lt;h2 id=&quot;phụ-lục&quot;&gt;Phụ lục&lt;/h2&gt;

&lt;p&gt;Ở phần này mình sẽ chứng minh 2 quan sát được đề cập ở phần 3 của bài viết. Quan sát &lt;a href=&quot;#first-obs&quot;&gt;thứ nhất&lt;/a&gt; rất dễ thấy nếu bạn sử dụng các biến đổi đại số tuyến tính với lưu ý nếu ma trận hiệp phương sai của vector biến ngẫu nhiên X là $\Sigma$ thì ma trận hiệp phương sai của $AX$, với $A$ là một ma trận, sẽ là $A\Sigma A^T$). Để chứng minh được quan sát &lt;a href=&quot;#second-obs&quot;&gt;thứ hai&lt;/a&gt; mình cần chứng minh hai bổ đề sau:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bổ đề 1&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Nếu $A$ là một ma trận luỹ đẳng đối xứng với chiều $n\times n$ thì tồn tại một ma trận $U$ có chiều $n\times r$ với $r$ là hạng của ma trận $A$ sao cho $A = UU^T$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Chứng minh&lt;/em&gt;: Vì $A$ là một ma trận luỹ đẳng nên các trị riêng của ma trận $A$ chỉ có thể là 0 hoặc 1. Mặt khác vì $A$ cũng là ma trận đối xứng nên theo định lí phổ, $A = UDU^T$ với $D$ là một ma trận chéo chứa các trị riêng của ma trận $A$ và ma trận $U$ là ma trận trực giao với các cột là các vector riêng tương ứng với trị riêng. Mặt khác, do $r$ là hạng của ma trận $A$ nên có tất cả $r$ trị riêng bằng 1. Vậy nên nếu bỏ tất cả các trị riêng bằng 0 và các vector riêng tương ứng, ta sẽ thu được ma trận $D$ mới đúng bằng với ma trận đơn vị có chiều là $r\times r$ và ma trận $U$ mới có chiều là $n\times r$ mà vẫn đảm bảo $A = UDU^T = UIU^T = UU^T$. Từ đó, ta có điều phải chứng minh. $\blacksquare$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bổ đề 2&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Nếu ta có $Z$ là một vector thuộc $\mathbb{R}^n$ gồm các biến ngẫu nhiên độc lập lấy từ phân phối chuẩn chuẩn hoá (hay nói cách khác, $Z\sim\mathcal{N}(0, I)$ với $I$ là ma trận đơn vị có chiều $n\times n$) và $A$ (cũng số chiều $n\times n$) là một ma trận luỹ đẳng đối xứng. Khi đó, $Z^TAZ$ sẽ là biến ngẫu nhiên có phân phối $\chi^2$ với bậc tự do là hạng của ma trận $A$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Chứng minh&lt;/em&gt;: Sử dụng bổ đề 1, ta thấy tồn tại một ma trận trực giao $U$ có chiều $n\times r$ thoả mãn $A = UU^T$. Do đó, $Z^TAZ = Z^TUU^TZ$. Mặt khác, nếu đặt $Z’=U^TZ$ thì rõ ràng $Z’\sim\mathcal{N}(0, I)$ thuộc $\mathbb{R}^r$ với $r$ là bậc của ma trận $A$. Do đó, $Z^TAZ = Z’^TZ’$ chính là biến ngẫu nhiên $\chi^2$ với bậc tự do là hạng của ma trận $A$. $\blacksquare$&lt;/p&gt;

&lt;p&gt;Quay trở lại bài toán của chúng ta, nếu viết lại $S$ dưới dạng ma trận thì ta sẽ có $S = y^T(I-H)y$ với $I$ là ma trận đơn vị có số chiều là $n$, và $H = X(X^TX)^{-1}X^T$ (X, y được thiết kế tại ($\ref{eq:01}$)). Như vậy $S\sim\sigma^2\chi^2_r$ với $r$ là hạng của ma trận $I - H$. Mặt khác, dễ dàng chứng minh $I - H$ là ma trận luỹ đẳng đối xứng nên ta có&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
    rank(I - H) &amp;= trace(I - H) = trace(I) - trace(H) = n - trace(X(X^TX)^{-1}X^T) \\
    &amp;= m - trace((X^TX)^{-1}X^TX) = m - n - 1.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Do đó, ta có $\frac{S}{\sigma^2}\sim\chi^2_{m-n-1}$. Từ đó, ta chứng minh được quan sát &lt;a href=&quot;#second-obs&quot;&gt;thứ hai&lt;/a&gt;.&lt;/p&gt;</content><author><name>Pham Hoang Minh</name></author><category term="maths" /><category term="optimization" /><category term="statistics" /><summary type="html">Giới thiệu</summary></entry><entry><title type="html">Toán học đằng sau thuật toán Gradient Descent.</title><link href="http://localhost:4000/gradient-descent" rel="alternate" type="text/html" title="Toán học đằng sau thuật toán Gradient Descent." /><published>2021-06-02T00:00:00+07:00</published><updated>2021-06-02T00:00:00+07:00</updated><id>http://localhost:4000/gradient-descent</id><content type="html" xml:base="http://localhost:4000/gradient-descent">&lt;h2 id=&quot;giới-thiệu&quot;&gt;Giới thiệu&lt;/h2&gt;

&lt;p&gt;Chuyện là bây giờ mình sắp kết thúc năm ba rồi và đang trong quá trình chuẩn bị thực tập. Nhưng mà portfolio của mình yếu quá nên giờ mình sẽ quay trở lại viết blog tiếp. Rất may mắn là mình kiếm được một chủ đề không mấy là mới nhưng lại khá là quan trọng trong quá trình học của mình. Mặc dù trường mình có dạy phương pháp này nhưng giáo viên dạy rất là khô khan và lan man (đây cũng là động lực chính để mình viết blog này). Mình mong bài viết này tuy ngắn nhưng sẽ bao quát hết được ý tưởng đằng sau của thuật toán và các khảo sát lí thuyết của thuật toán về tốc độ hội tụ.&lt;/p&gt;

&lt;p&gt;Thuật toán Gradient Descent lần đầu tiên được tìm ra bởi nhà toán học nổi tiếng Cauchy (cũng là nhân vật ám ảnh với các bạn học sinh cấp 2 với những bài toán bất đẳng thức khó). Sau đó phương pháp này được nghiên cứu bởi Haskell Cury (cha đẻ của ngôn ngữ lập trình Haskell, đi đầu về phong cách lập trình hướng hàm (functional programming)) và từ đó, Gradient Descent khá nổi tiếng trong giới khoa học dưới cái tên steepest descent.&lt;/p&gt;

&lt;p&gt;Bài blog này sẽ được chia làm ba phần với mức độ khó tăng dần: phần đầu mình sẽ giới thiệu về ý tưởng đằng sau thuật toán Gradient Descent, phần hai mình sẽ chứng minh tại sao thuật toán Gradient Descent có thể hoạt động được, và phần cuối sẽ khảo sát về tốc độ hội tụ của thuật toán.&lt;/p&gt;

&lt;h2 id=&quot;ý-tưởng-của-thuật-toán&quot;&gt;Ý tưởng của thuật toán&lt;/h2&gt;

&lt;p&gt;Hồi xưa khi còn học cấp 3, mình được các thầy cô giảng về cách tìm cực trị bằng việc sử dụng đạo hàm. Ví dụ như tìm giá trị nhỏ nhất của hàm số $y=x^2$ trên tập số thực chẳng hạn. Bước đầu tiên cần làm đó chính là tính đạo hàm của nó, đó là $y’=2x$. Rồi sau đó giải phương trình $y’=0$ để tìm ra các giá trị cực trị x (hàm số này có 1 điểm cực trị là x=0). Tiếp đó, bạn phải khảo sát hàm số trên các khoảng được chia bởi các điểm cực trị (trong trưởng hợp này là $(-\infty, 0)$ và $(0, \infty)$). Cuối cùng, sau khi khảo sát xong bạn kết luận $y$ đạt GTNN khi $x=0$. Cách làm này khá hợp lí nhưng có nhiều bước quá dư thừa: không nhất thiết phải khảo sát hàm số thì mới tìm ra được GTNN (tại sao phải khảo sát toàn bộ hàm số chỉ để giải phương trình $y’=0$ cho ra kết quả?). Nếu bạn là một học sinh Chuyên toán (hoặc trường bạn không giảm tải chương này) thì bạn sẽ được biết đến một cách giải ngắn hơn hiệu quả hơn dựa trên một định lí đề cập Sách giáo khoa:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Định lí 1:&lt;/strong&gt; Cho một hàm số $f(x)$ liên tục trên $\mathbb{R}$.&lt;/p&gt;

  &lt;p&gt;Khi đó, điểm $x_0$ được gọi là:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;điểm cực tiểu nếu $f’(x_0) = 0$ và $f”(x_0) &amp;gt; 0$.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;điểm cực đại nếu $f’(x_0)=0$ và $f”(x_0) &amp;lt; 0$.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Định lí này giúp cho việc tìm cực tiểu hoặc cực đại của hàm số trở nên dễ dàng hơn không chỉ về mặt toán học mà còn về mặt lập trình (trên thực tế, các thuật toán tối ưu hoá trong lĩnh vực Khoa học Máy tính đều cố gắng chuyển hoá các bài toán cực trị thành các bài toán giải phương trình và hệ phương trình rồi sử dụng các phương pháp lặp để tìm ra nghiệm của bài toán). Định lí 1 là tiền đề cho phương pháp Newton-Raphson trong việc tìm giá trị cực tiểu (hoặc cực đại) của một hàm số lồi (hoặc lõm). Ý tưởng rất đơn giản: giả sử $f$ đã được chứng minh là một hàm lồi (hoặc lõm), điểm tối ưu của $f$ chính là nghiệm của phương trình $f’(x) = 0$ và cũng là điểm hội tụ của dãy số $(x_n)$ xác định bởi:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left\{\begin{array}{l}
        x_0\\
        x_{n + 1} = x_n - \dfrac{f'(x_n)}{f''(x_n)}\forall n\geq 0
    \end{array}\right.&lt;/script&gt;

&lt;p&gt;Phương pháp Newton-Raphson hiệu quả về mặt lí thuyết và có nhiều ứng dụng trong thực tế (như chức năng giải nghiệm trên các máy tính cầm tay có mặt trên thị trường). Tuy nhiên, khi nói đến khía cạnh ứng dụng, phương pháp này gặp phải 2 trở ngại lớn:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Phương pháp này đòi hỏi việc chọn $x_0$ rất gần so với nghiệm thực tế của bài toán, nếu không dãy số thiết lập rất dễ bị phân kì.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Trong các bài toán hàm số đa biến, việc này đòi hỏi phải tính nghịch đảo của ma trận Hessian. Tính đến hiện tại, các thuật toán tính nghịch đảo của ma trận đòi hỏi độ phức tạp thời gian và không gian rất lớn. Do đó, tuy tốc độ hội tụ của phương pháp Newton-Raphson nhanh, bộ nhớ và thời gian thực thi của phương pháp có thể sẽ hết trước khi bài toán hội tụ.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Có thuật toán nào chỉ cần tìm ra đạo hàm bậc 1 có thể suy ra được nghiệm tối ưu của hàm $f$ (nếu $f$ là hàm lồi hoặc lõm)? Đó là ý tưởng chính của thuật toán Gradient Descent. Phát biểu của thuật toán rất đơn giản như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cho hàm số lồi $f$ và dãy số $(x_n)$ được định nghĩa bởi:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left\{\begin{array}{l}
        x_0\\
        x_{n + 1} = x_n - \alpha_n * f'(x)\forall n\geq 0
    \end{array}\right.&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Khi đó tồn tại dãy số không âm $(\alpha_n)$ để dãy số $(x_n)$ hội tụ về điểm cực tiểu.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Về chứng minh toán học, mình sẽ đề cập ở phần sau. Ở phần này, mình muốn cho các bạn thấy ý tưởng đằng sau thuật toán này. Ý tưởng xuất phát từ một quan sát rất tầm thường với các bạn học sinh cấp 3: nếu f giảm trên đoạn $[a, b]$ thì $f’(x)\leq 0$ và ngược lại nếu $f$ tăng trên đoạn $[a, b]$ thì $f’(x)\geq 0$. Vậy nếu chọn đại một điểm $(x_0, f(x_0))$ và $x_0$ nằm bên trái điểm cực tiểu thì $f’(x_0) &amp;lt; 0$ (giả sử f là hàm lồi) nên $x_1$ sẽ di chuyển về bên phải, tức là hướng về điểm cực tiểu (Minh hoạ bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/0*En4lt8S2kEwtSkjV.gif&quot; alt=&quot;gradient_descent_left&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ở hình này, người minh hoạ đang chỉnh &lt;span class=&quot;nv&quot;&gt;alpha_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.1 với mọi n &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;dòng đầu tiên&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

và chạy từng bước của thuật toán gradient descent. Như các bạn thấy, &lt;span class=&quot;k&quot;&gt;do

&lt;/span&gt;x0 nằm bên tay trái điểm cực tiểu nên x1 sẽ được cộng một lượng dương từ x0

và di chuyển về bên phải, tiến về điểm cực tiểu.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lí luận tương tự, nếu $x_0$ nằm bên tay phải của điểm cực tiểu thì $f’(x_0) &amp;gt; 0$ và $x_1$ sẽ đi về bên trái, cũng hướng về điểm cực tiểu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/864/1*8HJvJ1bmPukRvbWZaMt-bQ.gif&quot; alt=&quot;gradient_descent_right&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ở hình này, người minh hoạ đang chỉnh &lt;span class=&quot;nv&quot;&gt;alpha_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.01 với mọi n &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;dòng đầu tiên&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

và chạy từng bước của thuật toán gradient descent. Như các bạn thấy, &lt;span class=&quot;k&quot;&gt;do

&lt;/span&gt;x0 nằm bên tay phải điểm cực tiểu nên x1 sẽ bị trừ đi một lượng từ x0

và di chuyển về bên trái, tiến về điểm cực tiểu.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Nếu bạn tinh ý, bạn sẽ thắc mắc liệu $x_n$ có thể di chuyển quá điểm cực tiểu không? Câu trả lời là có nếu bạn chọn dãy $(\alpha_n)$ không phú hợp ($\alpha_n$ quá lớn với mọi n). Tưởng tượng dãy $(\alpha_n)$ không phù hợp giống như bạn chơi cầu trượt khi chạm đến đáy có bôi nhớt bạn sẽ phải trượt ra thêm một đoạn nữa (và té dập mặt) thì trường hợp này cũng giống như vậy. Chính vì điều này mà $(\alpha_n)$ có một tên gọi khác là tốc độ học (tên tiếng Anh là learning rate). Hình phía dưới minh hoạ cho việc gradient descent thất bại khi chọn tốc độ học quá lớn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/1*Q-2Wh0Xcy6fsGkbPFJvMhQ.gif&quot; alt=&quot;gradient_descent_fail&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ở hình này, từ trái qua phải là các trường hợp chọn tốc độ học alpha_n bằng 0.03, 0.4, 1.02.

Nếu chọn 0.03 thì x_n sẽ đi chậm về phía điểm cực tiểu. 

Nếu chọn 0.4 thì x_n sẽ nhanh chóng hội tụ về điểm cực tiểu.

Nếu chọn 1.02 thì x_n sẽ lạng lách và đi càng ngày càng xa điểm cực tiểu.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Vậy làm sao để chọn tốc độ học phù hợp? Đây là một câu hỏi mở cho thuật toán Gradient Descent. Thông thường, tốc độ học sẽ nằm vào trong khoảng $(0, 1)$ và thường được cố định bằng 1 hằng số qua các vòng lặp. Tuy nhiên, cách này không hiệu quả cho các ứng dụng lớn có dữ liệu nhiều chiều. Chính vì thế, đã có rất nhiều thuật toán giải quyết vấn đề chọn $(\alpha_n)$ bằng việc xây dựng dãy $(\alpha_n)$ thích nghi với từng vòng lặp. Nhưng đó sẽ là chủ đề cho các bài viết sau. Trước khi bước vào phần khảo sát về tính hội tụ của thuật toán, mình xin để phiên bản Gradient Descent cho các hàm số đa biến:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Cho $f:\mathbb{R^n}\to\mathbb{R}$ là một hàm lồi theo mọi biến và dãy vector $(x_n)$ được định nghĩa bởi:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\left\{\begin{array}{l}
        x_0\\
        x_{n + 1} = x_n - \alpha_n * \nabla f\forall n\geq 0
    \end{array}\right.&lt;/script&gt;

  &lt;p&gt;Khi đó tồn tại dãy số không âm $(\alpha_n)$ để dãy vector $(x_n)$ hội tụ về điểm cực tiểu.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;chứng-minh-sự-hội-tụ-của-thuật-toán&quot;&gt;Chứng minh sự hội tụ của thuật toán&lt;/h2&gt;

&lt;p&gt;Trước hết, chúng ta cần phải chứng minh dãy số xây dựng bởi thuật toán trên hội tụ với dãy $(\alpha_n)$ thích hợp. Tất cả các chứng minh kế tiếp đây của mình sẽ thực hiện trên hàm 1 biến nhưng có thể mở rộng ra hàm nhiều biến. Mở rộng thế nào sẽ là phần của các bạn.&lt;/p&gt;

&lt;p&gt;Để có thể chứng minh sự hội tụ của thuật toán Gradient Descent, chúng ta cần đến một định lí tổng quát hơn có tên là định lí Bolzano-Weierstrass. Định lí được phát biểu như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nếu dãy $(x_n)$ đơn điệu tăng và bị chặn trên hoặc đơn điệu giảm và bị chặn dưới thì dãy $(x_n)$ hội tụ.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Chứng minh&lt;/em&gt;&lt;/strong&gt;: Mình sẽ chứng minh với trường hợp dãy đơn điệu tăng. Chứng minh tương tự với dãy đơn điệu giảm.&lt;/p&gt;

  &lt;p&gt;Do $(x_n)$ là một dãy bị chặn trên nên dãy $(x_n)$ có chặn trên đúng (chặn trên nhỏ nhất). Kí hiệu là $c$. Như vậy, với mọi số $\epsilon &amp;gt; 0$, phải tồn tại một số $n_0$ nào đó thoả $x_{n_0} &amp;gt; c - \epsilon$ (nếu không $c - epsilon$ sẽ là chặn trên đúng của dãy, vô lí). Do $(x_n)$ là một dãy tăng nên ta có thể suy ra được:&lt;/p&gt;

  &lt;p&gt;với mọi $\epsilon &amp;gt; 0$, tồn tại $n_0$, sao cho với mọi $n &amp;gt; n_0$: $c - \epsilon &amp;lt; x_{n_0} &amp;lt; x_n &amp;lt; c &amp;lt; c + \epsilon\Leftrightarrow |x_n - c| &amp;lt; \epsilon$.&lt;/p&gt;

  &lt;p&gt;Suy ra được $\lim_{n\to\infty} x_n = c$. Do đó ta có điều phải chứng minh.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Quay về bài toán chính, dễ thấy $x_{n+1} - x_{n} = -\alpha_n f’(x_n)$. Nếu $x_0$ nhỏ hơn điểm cực trị và ta xây dựng dãy $(\alpha_n)$ sao cho $\alpha_n &amp;lt; \frac{x_n - x^*}{f’(x_n)}$ với $x^*$ là điểm cực trị thì dãy $(x_n)$ sẽ trở thành 1 dãy tăng và bị chặn trên bởi $x^*$. Như vậy $(x_n)$ sẽ hội tụ về điểm cực tiểu. Trường hợp $x_0$ lớn hơn điểm cực tiểu với cách tương tự ta cũng có thể thu được dãy $(x_n)$ hội tụ. Vậy suy ra điều phải chứng minh. Như vậy, có thể thấy với dãy $(\alpha_n)$ phù hợp, thuật toán Gradient Descent sẽ đảm bảo sự hội tụ về điểm tối ưu.&lt;/p&gt;

&lt;h2 id=&quot;tốc-độ-hội-tụ-của-gradient-descent&quot;&gt;Tốc độ hội tụ của Gradient Descent&lt;/h2&gt;

&lt;p&gt;Một trong những cách đánh giá tốc độ hội tụ của một dãy số là sử dụng kí hiệu $\mathcal{O}$:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Một dãy số $(x_n)$ hội tụ đến $L$ với tốc độ $\mathcal{O}(f(n))$ nếu và chỉ nếu $|x_n - L| = \mathcal{O}(f(n))$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Chúng ta giả sử hàm $f$ là một hàm đa biến lồi và liên tục L-Lipschitz (tức là $||\nabla f(y) - \nabla f(x)||\leq L ||y-x||$, cần phải giả sử đây là hàm L-Lipschitz để vector gradient không thay đổi đột ngột trong quá trình thực thi). Như vậy, tốc độ hội tụ của thuật toán Gradient Descent là $\mathcal{O}\left(\dfrac{1}{n}\right)$ nếu chọn $\alpha_n = \alpha &amp;lt; \dfrac{1}{L}$ với n là số lần chạy thuật toán Gradient Descent.&lt;/p&gt;

&lt;p&gt;Để chứng minh được điều này, chúng ta cần chứng minh bất đẳng thức sau:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_n) - f(x^*)\leq\dfrac{\|x_0-x^*\|}{2n\alpha}&lt;/script&gt;

&lt;p&gt;với $x^*$ là điểm cự tiểu, $n$ là số vòng lần thực thi Gradient Descent, và $\alpha$ là tốc độ học.&lt;/p&gt;

&lt;p&gt;Do $f$ là một hàm L-Lipschitz nên&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_n)\leq f(x_{n-1})+\nabla f(x_{n-1})^T(x_n-x_{n-1})+\dfrac{L}{2}\|x_{n}-x_{n-1}\|_2^2&lt;/script&gt;

&lt;p&gt;Mà $x_n - x_{n-1} = -\alpha\nabla f(x_{n-1})$ nên&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
        f(x_n)&amp;\leq f(x_{n-1}) + \alpha\|\nabla f(x_{n-1})\|_2^2 + \dfrac{L}{2}\alpha^2\|\nabla f(x_{n-1})\|_2^2\nonumber\\
        &amp;=f(x_{n-1}) + \alpha\|\nabla f(x_{n-1})\|_2^2\left(1 - \dfrac{L}{2}\alpha\right)\nonumber\\
        &amp;\leq f(x_{n-1}) - \dfrac{\alpha}{2}\|\nabla f(x_{n-1})\|_2^2\label{eq:1}
    \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Do $f$ là một hàm đa biến lồi nên&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
        f(x*)\geq f(x_{n-1})+\nabla f(x_{n-1})^T(x^* - x_{n-1})\nonumber\\
        \Leftrightarrow f(x_{n-1})\leq f(x^*)+\nabla f(x_{n-1})^T(x_{n-1} - x^*)\label{eq:2}
    \end{align}&lt;/script&gt;

&lt;p&gt;Thay $(\ref{eq:2})$ vào $(\ref{eq:1})$, ta có:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
        f(x_n) &amp;\leq f(x^*)+\nabla f(x_{n-1})^T(x_{n-1} - x^*) - \dfrac{\alpha}{2}\|\nabla f(x_{n-1})\|_2^2\nonumber\\
        &amp;=f(x^*) + \dfrac{1}{2\alpha}\left(2\alpha\nabla f(x_{n-1})^T(x_{n-1} - x^*) - \alpha^2\|\nabla f(x_{n-1})\|_2^2\right)\nonumber\\
        &amp;=f(x^*) + \dfrac{1}{2\alpha}(\|x_{n-1} - x^*\|_2^2 - \|x_{n-1} - x^* - \alpha\nabla f(x_{n-1})\|_2^2)\nonumber\\
        &amp;=f(x^*) + \dfrac{1}{2\alpha}(\|x_{n-1} - x^*\|_2^2 - \|x_n - x^*\|_2^2)\nonumber\\
        \Leftrightarrow &amp;f(x_n) - f(x^*)\leq\dfrac{1}{2\alpha}(\|x_{n-1} - x^*\|_2^2 - \|x_n - x^*\|_2^2)\nonumber
    \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Từ đây, ta có được&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^nf(x_i) - f(x^*)\leq\dfrac{1}{2\alpha}(\|x_0 - x^*\|_2^2 - \|x_n - x^*\|_2^2)&lt;/script&gt;

&lt;p&gt;Vì $f(x_i)$ giảm khi i tăng nên $n(f(x_n) - f(x^*))\leq\dfrac{1}{2\alpha}(||x_0 - x^*||_2^2 - ||x_n - x^*||_2^2)\leq\dfrac{1}{2\alpha}||x_0 - x^*||_2^2$ hay $f(x_n) - f(x^*)\leq\dfrac{||x_0 - x^*||_2^2}{2n\alpha}$. Từ đây, ta có được điều phải chứng minh. Như vậy, trong trường hợp trung bình, thuật toán Gradient Descent có tốc độ hội tụ $\mathcal{O}\left(\dfrac{1}{n}\right)$ với $\alpha\leq\dfrac{1}{L}$. Tuy tốc độ này không nhanh bằng việc sử dụng phương pháp Newton-Raphson (hội tụ bậc 2), thuật toán Gradient Descent ổn định hơn do chỉ tính đạo hàm một lần và vẫn đảm bảo tốc độ hội tụ ở ngưỡng chấp nhận được.&lt;/p&gt;

&lt;h2 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h2&gt;

&lt;p&gt;Thuật toán Gradient Descent nhìn chung là một thuật toán ổn định với sự đảm bảo trong việc tìm ra nghiệm tối ưu sau một số lượng vòng lặp đủ lớn. Ý tưởng của thuật toán Gradient Descent cũng rất đơn giản trực tiếp nhưng vẫn có thể đánh bại các thuật toán tối ưu hoá lâu đời. Điều đáng tiếc là thuật toán Gradient Descent có tốc độ hội tụ chậm hơn các phương pháp lâu đời khác và việc phải lựa chọn tốc độ học sao cho phù hợp đôi khi đòi hỏi sự dày công thử nghiệm (việc tính hệ số Lipschitz cũng đòi hỏi độ phức tạp khá lớn). Chính vì vậy, nhiều phiên bản cải tiến hơn của Gradient Descent đã ra đời nhưng đó là chủ đề cho các bài viết tiếp theo.&lt;/p&gt;</content><author><name>Pham Hoang Minh</name></author><category term="maths" /><category term="optimization" /><category term="convergence-analysis" /><summary type="html">Giới thiệu</summary></entry><entry><title type="html">Vietnam AQI Visualization</title><link href="http://localhost:4000/aqi-viz" rel="alternate" type="text/html" title="Vietnam AQI Visualization" /><published>2021-06-01T00:00:00+07:00</published><updated>2021-06-01T00:00:00+07:00</updated><id>http://localhost:4000/aqi-viz</id><content type="html" xml:base="http://localhost:4000/aqi-viz">&lt;!--&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &lt;title&gt;Vietnam's AQI&lt;/title&gt;
    &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot;&gt; 
    &lt;link href=&quot;https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700&amp;display=swap&quot; rel=&quot;stylesheet&quot;&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;assets/style.css&quot;&gt;
&lt;/head&gt;--&gt;
&lt;body&gt;
    &lt;div id=&quot;main-page&quot;&gt;
        &lt;div id=&quot;title&quot; class=&quot;bar-graph-panel&quot;&gt;
            &lt;h1&gt;Vietnam's air condition&lt;/h1&gt;
        &lt;/div&gt;
        &lt;div id=&quot;world-rank&quot; class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;How polluted is Vietnam's air in the world?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Vietnam stands at position 21st among the most 100 air-polluted countries in 2020.&lt;/p&gt;
            &lt;br /&gt;
            &lt;div class=&quot;year-option-wrapper&quot; id=&quot;world&quot;&gt;
                &lt;div class=&quot;year-option&quot;&gt;2018&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2019&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2020&lt;/div&gt;
            &lt;/div&gt;
            &lt;svg id=&quot;rank-world-graph&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        
        &lt;div id=&quot;asean-rank&quot; class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;How polluted is Vietnam's air in ASEAN?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Vietnam stands at position 3rd among ASEAN countries in terms of AQI in 2020.&lt;/p&gt;
            &lt;br /&gt;
            &lt;div class=&quot;year-option-wrapper&quot; id=&quot;asean&quot;&gt;
                &lt;div class=&quot;year-option&quot;&gt;2018&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2019&lt;/div&gt;
                &lt;div class=&quot;year-option&quot;&gt;2020&lt;/div&gt;
            &lt;/div&gt;
            &lt;svg id=&quot;rank-asia-graph&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        
        &lt;div class=&quot;map-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;AQI of some cities in Vietnam&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Most cities in Vietnam belongs to moderate and unhealthy groups.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;map&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;Why is it the case?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;Is it due to the reduction of forest area? No! The forest area is still increasing.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;forest&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;Why is it the case?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;However, the amount of carbon dioxide (CO2) is still increasing.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;co2&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;Why is it the case?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;The only reason is from the human activity. Below is the chart for the amount of CO2 emission from three sources: oil (red), gas (blue), coal (green).&lt;/p&gt;
            &lt;p&gt;Vietnam releases at least 10 million tons of CO2 each year. Not surprisingly, coal emits the most amount of CO2 for it is used to produce electricity and gasoline.&lt;/p&gt;
            &lt;br /&gt;
            &lt;svg id=&quot;human&quot; width=&quot;100%&quot; height=&quot;70%&quot;&gt;&lt;/svg&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;What are possible solutions?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;The only solution is to less depend on fossil fuel products. For individuals, this can be:&lt;/p&gt;
            &lt;div class=&quot;solution-wrapper&quot;&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://images.vexels.com/media/users/3/128905/isolated/preview/b04ee8fc260d4c6918b67e960ae3b8f5-tour-bus-silhouette-by-vexels.png&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;
                    &lt;p&gt;Using public transports.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Emoji_u2600.svg/1200px-Emoji_u2600.svg.png&quot; alt=&quot;&quot; width=&quot;150&quot; height=&quot;150&quot; /&gt;
                    &lt;p&gt;Utilize solar and other renewable energy.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://www.freeiconspng.com/thumbs/fuel-icon/fuel-pump-icon-23.png&quot; alt=&quot;&quot; width=&quot;150&quot; height=&quot;150&quot; /&gt;
                    &lt;p&gt;Using environmental-friendly fuels (such as bilogical fuels).&lt;/p&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;bar-graph-panel&quot;&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;h2&gt;What are possible solutions?&lt;/h2&gt;
            &lt;br /&gt;
            &lt;p&gt;For government, this can be:&lt;/p&gt;
            &lt;div class=&quot;solution-wrapper&quot;&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://icons-for-free.com/iconfiles/png/512/svg+lab+microscope+science+icon-1320190754102964758.png&quot; alt=&quot;&quot; /&gt;
                    &lt;p&gt;Research and discover more efficient energy sources.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://images.vexels.com/media/users/3/128905/isolated/preview/b04ee8fc260d4c6918b67e960ae3b8f5-tour-bus-silhouette-by-vexels.png&quot; alt=&quot;&quot; /&gt;
                    &lt;p&gt;Improve transport systems and infrastructures.&lt;/p&gt;
                &lt;/div&gt;
                &lt;div class=&quot;solution-panel&quot;&gt;
                    &lt;img src=&quot;https://icon-library.com/images/tax-icon-png/tax-icon-png-10.jpg&quot; alt=&quot;&quot; /&gt;
                    &lt;p&gt;Implement environmental taxes.&lt;/p&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;div id=&quot;footer&quot;&gt;
                &lt;p&gt;
                    This project is a small project conducted by a group of students 
                    from the International University, so it is not funded by any organizations.
                &lt;/p&gt;
                &lt;p&gt;
                    Full source for this project is published 
                    &lt;a href=&quot;https://github.com/minhrongcon2000/vn-aqi-viz&quot; target=&quot;_blank&quot;&gt;
                        here
                    &lt;/a&gt;
                &lt;/p&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;script src=&quot;https://d3js.org/d3.v6.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;https://d3js.org/d3-path.v2.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;https://d3js.org/d3-shape.v2.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/scrollytell.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/utils.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/create_map.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/create_bar_chart.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;assets/create_line_chart.js&quot;&gt;&lt;/script&gt;
&lt;/body&gt;</content><author><name>Pham Hoang Minh</name></author><category term="data-visualization" /><summary type="html">Vietnam's air condition How polluted is Vietnam's air in the world? Vietnam stands at position 21st among the most 100 air-polluted countries in 2020. 2018 2019 2020 How polluted is Vietnam's air in ASEAN? Vietnam stands at position 3rd among ASEAN countries in terms of AQI in 2020. 2018 2019 2020 AQI of some cities in Vietnam Most cities in Vietnam belongs to moderate and unhealthy groups. Why is it the case? Is it due to the reduction of forest area? No! The forest area is still increasing. Why is it the case? However, the amount of carbon dioxide (CO2) is still increasing. Why is it the case? The only reason is from the human activity. Below is the chart for the amount of CO2 emission from three sources: oil (red), gas (blue), coal (green). Vietnam releases at least 10 million tons of CO2 each year. Not surprisingly, coal emits the most amount of CO2 for it is used to produce electricity and gasoline. What are possible solutions? The only solution is to less depend on fossil fuel products. For individuals, this can be: Using public transports. Utilize solar and other renewable energy. Using environmental-friendly fuels (such as bilogical fuels). What are possible solutions? For government, this can be: Research and discover more efficient energy sources. Improve transport systems and infrastructures. Implement environmental taxes. This project is a small project conducted by a group of students from the International University, so it is not funded by any organizations. Full source for this project is published here</summary></entry></feed>